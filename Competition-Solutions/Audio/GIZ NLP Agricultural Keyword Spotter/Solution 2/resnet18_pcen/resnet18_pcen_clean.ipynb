{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"resnet18_pcen_clean.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b00d01a3eb2e494e8202509ffe21dab8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ca4c7571d7d43f2991b5c6c55948c49","IPY_MODEL_8fea14e1e8ae44de9b11496fc9c37e3a"],"layout":"IPY_MODEL_447854096bc24952985b2a1199e19e75"}},"5ca4c7571d7d43f2991b5c6c55948c49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_16f4beb9416549a8831bf77249268469","max":4709,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c8b3548cb91442b9c88673446d91b40","value":4709}},"8fea14e1e8ae44de9b11496fc9c37e3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3debd91c77c24df3aabbfc1a8e642a2f","placeholder":"​","style":"IPY_MODEL_efff836f8be349dc8d0c288dc3375c3d","value":" 4709/4709 [1:07:39&lt;00:00,  1.16it/s]"}},"447854096bc24952985b2a1199e19e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f4beb9416549a8831bf77249268469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c8b3548cb91442b9c88673446d91b40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"3debd91c77c24df3aabbfc1a8e642a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efff836f8be349dc8d0c288dc3375c3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2df9646852ee449db8001e9e81cdd340":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8e991f74a1e4a6a9f8d56e3e4a0b71a","IPY_MODEL_1b7fe1ef9764421f988004a9ae69df18"],"layout":"IPY_MODEL_5a9b358f45554c28ac1c149311dd5dff"}},"b8e991f74a1e4a6a9f8d56e3e4a0b71a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_5dd6754dee66409da4e94042321ced10","max":1017,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b29c36f086c24633b78c08f43349fad2","value":1017}},"1b7fe1ef9764421f988004a9ae69df18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc7a8d94ec8a4ae7a6840c529d33d2f0","placeholder":"​","style":"IPY_MODEL_43fda713176a4d8ca7eb76222d03a456","value":" 1017/1017 [34:21&lt;00:00,  2.03s/it]"}},"5a9b358f45554c28ac1c149311dd5dff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd6754dee66409da4e94042321ced10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b29c36f086c24633b78c08f43349fad2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"fc7a8d94ec8a4ae7a6840c529d33d2f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43fda713176a4d8ca7eb76222d03a456":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12618002b54146b485722a50d5433e91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e89e4a71427143fc807dfea473b5951d","IPY_MODEL_8d6ab6e0f5a14af7ada8d7761bdb3deb"],"layout":"IPY_MODEL_635acd7387714fe4bf600a3d100bbdee"}},"e89e4a71427143fc807dfea473b5951d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_f00c41f347254fb2b8408afe9bd18d8f","max":46827520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_467178f4de654ad1a7467011c315c699","value":46827520}},"8d6ab6e0f5a14af7ada8d7761bdb3deb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48a28b28baa5408ca74dbfe568658379","placeholder":"​","style":"IPY_MODEL_af94b0bec4764ba7b8ed0babd00dbb4e","value":" 44.7M/44.7M [00:00&lt;00:00, 103MB/s]"}},"635acd7387714fe4bf600a3d100bbdee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00c41f347254fb2b8408afe9bd18d8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"467178f4de654ad1a7467011c315c699":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"48a28b28baa5408ca74dbfe568658379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af94b0bec4764ba7b8ed0babd00dbb4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"YaSBI1FrBPbO"},"source":["# Drive and env"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wX2LTul6i_5e","executionInfo":{"elapsed":31675,"status":"ok","timestamp":1606852700704,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"47218715-7756-42f7-ec47-96c70bfe9ea2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4-rDTVFBRs-","executionInfo":{"elapsed":9272,"status":"ok","timestamp":1606852706576,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"3561f90d-a3ec-45f9-8f05-ac793bead3d5"},"source":["%pip install git+https://github.com/Mo5mami/wtfml.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/Mo5mami/wtfml.git\n","  Cloning https://github.com/Mo5mami/wtfml.git to /tmp/pip-req-build-22ouadt9\n","  Running command git clone -q https://github.com/Mo5mami/wtfml.git /tmp/pip-req-build-22ouadt9\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.6/dist-packages (from wtfml==0.0.4) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.1->wtfml==0.0.4) (1.18.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.1->wtfml==0.0.4) (0.17.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.1->wtfml==0.0.4) (1.4.1)\n","Building wheels for collected packages: wtfml\n","  Building wheel for wtfml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wtfml: filename=wtfml-0.0.4-cp36-none-any.whl size=12544 sha256=958ad14b4d92c14b5e0e47eddba4000210502873fc0c407f8415e1702813e6c3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w26hv0ej/wheels/5e/37/fe/2ae653f8b5b64673abba424c23de5406ac217f7c3df4aa905d\n","Successfully built wtfml\n","Installing collected packages: wtfml\n","Successfully installed wtfml-0.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0vy-CXECUEx","executionInfo":{"elapsed":16870,"status":"ok","timestamp":1606852714183,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"a13154e1-ff25-485b-c30f-b6217e172dbc"},"source":["%pip install torchaudio librosa pretrainedmodels albumentations==0.4.6"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/23/6b54106b3de029d3f10cf8debc302491c17630357449c900d6209665b302/torchaudio-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 9.5MB/s \n","\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n","Collecting pretrainedmodels\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n","\u001b[?25hCollecting albumentations==0.4.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n","\u001b[K     |████████████████████████████████| 122kB 37.9MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.7.0+cu101)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.15.0)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.17.0)\n","Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n","Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.48.0)\n","Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.18.5)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.9)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.8.1+cu101)\n","Collecting munch\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 39.0MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (0.8)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (50.3.2)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n","Building wheels for collected packages: pretrainedmodels, albumentations\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60964 sha256=ba6f3d749ddbb6e668fefe89e1b6a9d3d1016ba4c30d777d59f6639b7ae12a2d\n","  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-cp36-none-any.whl size=65165 sha256=fd7ca48b2b78cfb977e3996fc307aa80cadc65c3e159dc52c54e8b3884512c60\n","  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n","Successfully built pretrainedmodels albumentations\n","Installing collected packages: torchaudio, munch, pretrainedmodels, imgaug, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.6 imgaug-0.4.0 munch-2.5.0 pretrainedmodels-0.7.4 torchaudio-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAavaqBYGuXU","executionInfo":{"elapsed":17642,"status":"ok","timestamp":1606852714964,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"a484b728-8dd6-4d35-fcce-5e1494af5d1c"},"source":["!pip freeze | grep torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch==1.7.0+cu101\n","torchaudio==0.7.0\n","torchsummary==1.5.1\n","torchtext==0.3.1\n","torchvision==0.8.1+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldjVWLvw-to9","executionInfo":{"elapsed":27014,"status":"ok","timestamp":1606852724345,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"102ffc51-0e67-4e3a-df8b-7d557af0d501"},"source":["from __future__ import print_function\n","import argparse\n","import sys\n","import os\n","import random\n","import librosa\n","from tqdm.notebook import tqdm\n","import scipy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torchaudio\n","import torchvision\n","from scipy.io import wavfile\n","import IPython.display as ipd\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","#from utils import one_hot_embedding\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,Dataset\n","from sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit\n","import albumentations\n","\n","from albumentations.pytorch.transforms import ToTensor\n","from google.colab.patches import cv2_imshow\n","from wtfml.utils import EarlyStopping\n","from wtfml.engine import Engine\n","import pretrainedmodels\n","from pretrainedmodels.models import nasnetamobile\n","import cv2\n","import gc\n","import math\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","from scipy.stats.mstats import gmean\n","from librosa.display import specshow\n","from sklearn.utils import class_weight\n","from torch.optim.lr_scheduler import _LRScheduler\n","import io\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n","  '\"sox\" backend is being deprecated. '\n","/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"51Dpvo2EIs2-"},"source":["def seed_all(seed_value):\n","    random.seed(seed_value) # Python\n","    np.random.seed(seed_value) # cpu vars\n","    torch.manual_seed(seed_value) # cpu  vars\n","    \n","    if torch.cuda.is_available(): \n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value) # gpu vars\n","        torch.backends.cudnn.deterministic = True  #needed\n","        torch.backends.cudnn.benchmark = False\n","num_seed=42\n","seed_all(num_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Enh73pkrJtb","executionInfo":{"elapsed":27010,"status":"ok","timestamp":1606852724352,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"21c5446e-a9a9-435f-81c4-dba21e6d6d20"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Dec  1 19:58:43 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x81-1ToVpEMh","executionInfo":{"elapsed":27353,"status":"ok","timestamp":1606852724701,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"10598ac2-6a3c-47ac-f87b-a5072ff6be7e"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Dec  1 19:58:43 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDZbdWcfGZA7","executionInfo":{"elapsed":27346,"status":"ok","timestamp":1606852724703,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"aa057864-5e12-4877-841b-56cb9262da66"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"F6hTOg7_BlR6"},"source":["# Utils and settings"]},{"cell_type":"markdown","metadata":{"id":"Up4hXqtdi4mE"},"source":["## general settings"]},{"cell_type":"code","metadata":{"id":"owtw_hGLIojb"},"source":["class Config:\n","  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  epochs=40\n","  random_state=42\n","  train_batchsize=4\n","  test_batchsize=4\n","  val_every=5\n","  print_every=20\n","  logdir=\"logs\"\n","  DATASET_PATH=\"audio_files\"\n","  DATASET2_PATH=\"latest_keywords\"\n","  DATASET3_PATH=\"nlp_keywords\"\n","  n_folds=10\n","  test_size=0.1\n","  lr=1.2*1e-4\n","  aftertrain_lr=2*1e-6\n","  min_lr=0.1*1e-4\n","  experiment_id=\"20-11-20_exp1\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYM0FspMi6Po"},"source":["## audio settings"]},{"cell_type":"code","metadata":{"id":"CcWj0gfA1FS4"},"source":["class AudioConfig:\n","    audio_length=3\n","    sr=44100\n","    fixed_sr=audio_length*sr\n","    hop_length = 276\n","    fmin = 20\n","    fmax = 8000\n","    n_mels = 64\n","    n_mfcc=13\n","    n_fft = n_mels*20\n","    min_seconds = 0.1\n","    WRAP_PAD_PROB = 0.5\n","    pad=450\n","    spec_aug_prob=0.8\n","    mixer_prob=0.0\n","    audio_crop_prob=0.5\n","    height=228\n","    width=400\n","    duration=3.5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofe5D8qVC8B1"},"source":["## plot functions"]},{"cell_type":"code","metadata":{"id":"rSw0i21SCRQ5"},"source":["def plot_signal(signals):\n","    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n","                             sharey=True, figsize=(20,5))\n","    axes.set_title(\"sig\")\n","    axes.plot(list(signals))\n","    \n","    \n","def plot_signals(signals):\n","    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n","                             sharey=True, figsize=(20,5))\n","    fig.suptitle('Time Series', size=16)\n","    i = 0\n","    for x in range(2):\n","        for y in range(5):\n","            axes[x,y].set_title(list(signals.keys())[i])\n","            axes[x,y].plot(list(signals.values())[i])\n","            axes[x,y].get_xaxis().set_visible(False)\n","            axes[x,y].get_yaxis().set_visible(False)\n","            i += 1\n","\n","def plot_fft(fft):\n","    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n","                             sharey=True, figsize=(20,5))\n","    fig.suptitle('Fourier Transforms', size=16)\n","    i = 0\n","    for x in range(2):\n","        for y in range(5):\n","            data = list(fft.values())[i]\n","            Y, freq = data[0], data[1]\n","            axes[x,y].set_title(list(fft.keys())[i])\n","            axes[x,y].plot(freq, Y)\n","            axes[x,y].get_xaxis().set_visible(False)\n","            axes[x,y].get_yaxis().set_visible(False)\n","            i += 1\n","\n","def plot_fbank(fbank):\n","    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n","                             sharey=True, figsize=(20,5))\n","    fig.suptitle('Filter Bank Coefficients', size=16)\n","    i = 0\n","    for x in range(2):\n","        for y in range(5):\n","            axes[x,y].set_title(list(fbank.keys())[i])\n","            axes[x,y].imshow(list(fbank.values())[i],\n","                    cmap='hot', interpolation='nearest')\n","            axes[x,y].get_xaxis().set_visible(False)\n","            axes[x,y].get_yaxis().set_visible(False)\n","            i += 1\n","\n","def plot_mfccs(mfccs):\n","    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n","                             sharey=True, figsize=(20,5))\n","    \n","    axes.set_title(\"mfcc\")\n","    \n","    specshow(mfccs,x_axis='time',y_axis='mel', \n","                             sr=AudioConfig.sr, hop_length=AudioConfig.hop_length,\n","                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n","    plt.colorbar(format='%+2.0f dB')\n","    \n","    plt.show()\n","def get_plot_mfccs(mfccs):\n","    \n","    fig,ax = plt.subplots(1)\n","    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n","    ax.axis('off')\n","    \n","    \n","    specshow(mfccs,x_axis=\"time\",y_axis=\"mel\", \n","                             sr=AudioConfig.sr,hop_length=AudioConfig.hop_length,\n","                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n","    ax.axis('off')\n","    \n","    image=io.BytesIO()\n","    fig.savefig(image,bbox_inches='tight',pad_inches=0.0)\n","    img=np.frombuffer(image.getvalue(), dtype='uint8')\n","    img = cv2.imdecode(img,cv2.IMREAD_COLOR)\n","    image.close()\n","    plt.close()\n","    return img\n","\n","\n","\n","def plot_class_dist(X):\n","    class_dis=X.groupby(\"label\")[\"fn\"].count()\n","    fig, ax = plt.subplots()\n","    \n","    ax.set_title('Class Distribution', y=1.08)\n","    ax.pie(class_dis, labels=class_dis.index, autopct='%1.1f%%',\n","          shadow=False, startangle=90)\n","    ax.axis('equal')\n","    plt.show()\n","\n","\n","def show_melspectrogram(mels, title='Log-frequency power spectrogram'):\n","    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n","                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n","                            fmin=conf.fmin, fmax=conf.fmax)\n","    plt.colorbar(format='%+2.0f dB')\n","    plt.title(title)\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dc_Ouje2gWOW"},"source":["## util functions"]},{"cell_type":"code","metadata":{"id":"PXrn6BrlB4tb"},"source":["\"\"\"\n","Read audio from a path\n","\"\"\"\n","\n","def read_wav(filepath):\n","  \n","  sample_rate, samples = wavfile.read(filepath)\n","  return sample_rate,np.array(samples)\n","\n","\"\"\"\n","Listen to audio sample\n","\"\"\"\n","def listen(samples,sample_rate):\n","  return ipd.Audio(samples, rate=sample_rate)\n","\n","\"\"\"\n","Wav loader for DatasetFolder using librosa\n","\"\"\"\n","def wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n","    sample,sr=librosa.load(path,sr=sr)\n","    result=torch.zeros(1,fixed_sr)\n","    length=min(fixed_sr,len(sample))\n","    result[0,:length]=torch.tensor(sample[:length])\n","    return (result,sr)\n","    #return (torch.tensor([sample],),sr)\n","\n","\"\"\"\n","Wav loader for DatasetFolder using torchaudio\n","\"\"\"\n","\n","def torch_wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n","    sample,sr=torchaudio.load_wav(path)\n","    result=torch.zeros(1,fixed_sr)\n","    length=min(fixed_sr,sample.shape[1])\n","    result[0,:length]=sample[0,:length]\n","    return (result,sr)\n","\n","\"\"\"\n","accuracy measure\n","\"\"\"\n","def accuracy(predictions,real):\n","    return (predictions==real).sum()*100/len(predictions)\n","\n","def enveloppe(sig,sr,threshhold):\n","  mask=[]\n","  sig=pd.Series(sig).apply(np.abs)\n","  sig_mean=sig.rolling(window=int(sr/10),min_periods=1,center=True).mean()\n","  for mean in sig_mean:\n","    if mean>threshhold:\n","      mask.append(True)\n","    else:  mask.append(False)\n","  return mask\n","\n","\n","def read_audio(file_path,top_db=60):\n","    min_samples = int(AudioConfig.min_seconds * AudioConfig.sr)\n","    y, sr = librosa.load(file_path, sr=AudioConfig.sr)\n","    trim_y, trim_idx = librosa.effects.trim(y,top_db=top_db,frame_length=AudioConfig.n_fft, hop_length=AudioConfig.hop_length)  # trim, top_db=default(60)\n","\n","    if len(trim_y) < min_samples:\n","        center = (trim_idx[1] - trim_idx[0]) // 2\n","        left_idx = max(0, center - min_samples // 2)\n","        right_idx = min(len(y), center + min_samples // 2)\n","        trim_y = y[left_idx:right_idx]\n","\n","        if len(trim_y) < min_samples:\n","            padding = min_samples - len(trim_y)\n","            offset = padding // 2\n","            trim_y = np.pad(trim_y, (offset, padding - offset), 'constant')\n","\n","    return trim_y\n","\n","def test_top_db(filepath,top_db,print_mask=True):\n","  sample=read_audio(filepath)\n","  plot_signal(sample)\n","  sample_test=read_audio(filepath,top_db=top_db)\n","  plot_signal(sample_test)\n","  if(print_mask):  return listen(sample_test,AudioConfig.sr)\n","  else : return listen(sample,AudioConfig.sr)\n","\n","\n","def audio_to_melspectrogram(audio):\n","    \n","    spectrogram = librosa.feature.melspectrogram(audio,\n","                                                 sr=AudioConfig.sr,\n","                                                 n_mels=AudioConfig.n_mels,\n","                                                 n_fft=AudioConfig.n_fft,\n","                                                 hop_length=AudioConfig.hop_length,\n","                                                 fmin=AudioConfig.fmin,\n","                                                 fmax=AudioConfig.fmax,\n","                                                 power=0.2\n","                                                 )\n","    \n","    spectogram=librosa.pcen(spectrogram * (2**31))\n","    spectrogram = spectrogram.astype(np.float32)\n","    return spectrogram\n","\n","def read_as_melspectrogram(file_path,time_stretch=1.0, pitch_shift=0.0,\n","                           debug_display=False):\n","    x = read_audio(file_path)\n","    if time_stretch != 1.0:\n","        x = librosa.effects.time_stretch(x, time_stretch)\n","\n","    if pitch_shift != 0.0:\n","        librosa.effects.pitch_shift(x, config.sampling_rate, n_steps=pitch_shift)\n","\n","    \n","    mels = audio_to_melspectrogram(x)\n","    if debug_display:\n","        import IPython\n","        IPython.display.display(IPython.display.Audio(x, rate=config.sampling_rate))\n","        show_melspectrogram(mels)\n","    return (mels,AudioConfig.sr)\n","\n","def mix_up(x, y):\n","        x = np.array(x, np.float32)\n","        lam = np.random.beta(1.0, 1.0)\n","        ori_index = np.arange(int(len(x)))\n","        index_array = np.arange(int(len(x)))\n","        np.random.shuffle(index_array)        \n","        \n","        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n","        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n","        \n","        return mixed_x, mixed_y\n","\n","def oversample(dataframe):\n","    X,y=RandomOverSampler(random_state=42).fit_sample(dataframe, dataframe[\"label\"])\n","    return pd.DataFrame(X,columns=dataframe.columns).reset_index(drop=True)\n","\n","def to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return torch.eye(num_classes, dtype=float)[y]\n","\n","\"\"\"\n","function to test the enveloppe\n","\"\"\"\n","def test_enveloppe(filepath,thresh=0.0005,print_mask=True):\n","  sample=read_audio(filepath)\n","  print(sample.max())\n","  plot_signal(sample)\n","  mask=enveloppe(sample,AudioConfig.sr,thresh)\n","  plot_signal(sample[mask])\n","  if(print_mask):  return listen(sample[mask],AudioConfig.sr)\n","  else : return listen(sample,AudioConfig.sr)\n","\n","def create_csv_dataset_from_path(dataset_path):\n","    classes=[classe for classe in os.listdir(dataset_path)]\n","    class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n","    idx_to_class={idx:classe for idx,classe in enumerate(classes)}\n","    \n","    path=[]\n","    target=[]\n","    for classe in classes:\n","        class_path=os.path.join(dataset_path,classe)\n","        for sample in os.listdir(class_path):\n","            path.append(os.path.join(class_path,sample))\n","            target.append(classe)\n","    \n","    dataset=pd.DataFrame(data={\"fn\":path,\"label\":target})\n","    dataset = dataset.sample(frac=1,random_state=42).reset_index(drop=True)\n","    return dataset,classes\n","\n","def onset_test(path):\n","    y=read_audio(path)\n","    times = librosa.times_like(audio_to_melspectrogram(y))\n","    onset_env = librosa.onset.onset_strength(y=y, sr=AudioConfig.sr,\n","                                         aggregate=np.median,\n","                                         n_fft=AudioConfig.n_fft,\n","                                        hop_length=AudioConfig.hop_length,\n","                                         fmax=8000, n_mels=160)\n","    print(onset_env.argmax())\n","    print(onset_env.shape)\n","    plt.plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,\n","           label='Median (custom mel)')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"672QOn7droRK"},"source":["## Transformation"]},{"cell_type":"markdown","metadata":{"id":"5Gh-yj2Xf-x3"},"source":["### spect transformation"]},{"cell_type":"code","metadata":{"id":"-3XhMhrcg0D0"},"source":["class ToMfcc(object):\n","    def __call__(self,image,**kwargs):\n","        return audio_to_melspectrogram(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssDjVP1RyStS"},"source":["def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6,**kwargs):\n","    \n","    X=X.transpose(1, 0, 2)\n","    mean = mean or X.mean()\n","    std = std or X.std()\n","    Xstd = (X - mean) / (std + eps)\n","    _min, _max = Xstd.min(), Xstd.max()\n","    norm_max = norm_max or _max\n","    norm_min = norm_min or _min\n","    if (_max - _min) > eps:\n","        \n","        V = Xstd\n","        V[V < norm_min] = norm_min\n","        V[V > norm_max] = norm_max\n","        V = 255 * (V - norm_min) / (norm_max - norm_min)\n","        V = V.astype(np.uint8)\n","    else:\n","        \n","        V = np.zeros_like(Xstd, dtype=np.uint8)\n","    return V\n","  \n","\n","class ToColor:\n","    def __init__(self,\n","                  mean=None,\n","                  std=None):\n","        self.mean=mean\n","        self.std = std\n","        \n","\n","    def __call__(self, image,**kwargs):\n","        return mono_to_color(image,\n","                            self.mean,\n","                            self.std,\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Uf34VjlyUYW"},"source":["class AudioCrop:\n","    def __init__(self,percentage=0.75):\n","        self.percentage=percentage\n","\n","    def __call__(self,image,**kwargs):\n","        \n","        perc=np.random.random()*(1-self.percentage)+self.percentage\n","        return albumentations.RandomCrop(image.shape[0],int(image.shape[1]*perc),p=1)(image=image)[\"image\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zl8-3gOxod7c"},"source":["class Onset:\n","    def __init__(self,size):\n","        self.size=size\n","\n","    def __call__(self,image,**kwargs):\n","        \n","        onset_env = librosa.onset.onset_strength(S=image)\n","        argmax=onset_env.argmax()\n","        return albumentations.Crop(x_min=argmax-self.size//2, y_min=0, x_max=argmax+self.size//2, y_max=AudioConfig.n_mels,p=1)(image=image)[\"image\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uz4KGA4aNIt4"},"source":["class PadToSize:\n","    def __init__(self, size, mode='constant'):\n","        \n","        self.size = size\n","        self.mode = mode\n","\n","    def __call__(self, image,**kwargs):\n","        if image.shape[1] < self.size:\n","            padding = self.size - image.shape[1]\n","            offset = padding // 2\n","            pad_width = ((0, 0), (offset, padding - offset))\n","            if self.mode == 'constant':    \n","                image = np.pad(image, pad_width,'constant', constant_values=0)\n","            else:\n","                image = np.pad(image, pad_width, 'wrap')\n","        return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h85CAKDO8Jrx"},"source":["class AudioPad:\n","    def __init__(self,percentage=0.10, mode='constant'):\n","        self.percentage=percentage\n","        self.mode=mode\n","    def __call__(self,image,**kwargs):\n","        return PadToSize(int(image.shape[1]*(self.percentage+1)),self.mode)(image=image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFK_xL-vPr3x"},"source":["class ImageStack:\n","    def __call__(self, image,**kwargs):\n","        delta = librosa.feature.delta(image)\n","        accelerate = librosa.feature.delta(image, order=2)\n","        image = np.stack([image, delta, accelerate], axis=-1)\n","        \n","        image = image.astype(np.float32)\n","        return image\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G99PGSMw_A3J"},"source":["def spec_augment(spec: np.ndarray,\n","                 num_mask=2,\n","                 freq_masking=0.15,\n","                 time_masking=0.20,\n","                 value=0):\n","    spec = spec.copy()\n","    num_mask = random.randint(1, num_mask)\n","    for i in range(num_mask):\n","        all_freqs_num, all_frames_num  = spec.shape\n","        freq_percentage = random.uniform(0.0, freq_masking)\n","\n","        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n","        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n","        f0 = int(f0)\n","        spec[f0:f0 + num_freqs_to_mask, :] = value\n","\n","        time_percentage = random.uniform(0.0, time_masking)\n","\n","        num_frames_to_mask = int(time_percentage * all_frames_num)\n","        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n","        t0 = int(t0)\n","        spec[:, t0:t0 + num_frames_to_mask] = value\n","    return spec\n","\n","\n","class SpecAugment:\n","    def __init__(self,\n","                 num_mask=2,\n","                 freq_masking=0.15,\n","                 time_masking=0.20):\n","        self.num_mask = num_mask\n","        self.freq_masking = freq_masking\n","        self.time_masking = time_masking\n","\n","    def __call__(self, image,**kwargs):\n","        return spec_augment(image,\n","                            self.num_mask,\n","                            self.freq_masking,\n","                            self.time_masking,\n","                            image.min())\n","        \n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sh6HCuLTht4F"},"source":["### Get transformation"]},{"cell_type":"code","metadata":{"id":"p3IpGNumrqfN"},"source":["def get_transforms(train, height,width,\n","                   wrap_pad_prob=0.5,\n","                   resize_scale=(1, 0.8),\n","                   resize_ratio=(1, 2.4),\n","                   resize_prob=0.4,\n","                   spec_num_mask=2,\n","                   spec_freq_masking=0.15,\n","                   spec_time_masking=0.20,\n","                   spec_prob=0.5):\n","    mean = (0.485, 0.456, 0.406)\n","    std = (0.229, 0.224, 0.225)\n","    if train:\n","      \n","        transforms = albumentations.Compose([\n","            \n","            \n","            \n","            albumentations.OneOf([albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"constant\"),p=0.5),\n","                                  albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=0.5),\n","                                    ],p=1),\n","            \n","            albumentations.Lambda(AudioCrop(percentage=0.9), p=AudioConfig.audio_crop_prob),\n","            \n","            albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=0.0),\n","            \n","            albumentations.Resize(height,width,p=1),\n","            albumentations.OneOf([albumentations.Lambda(SpecAugment(num_mask=2,freq_masking=0.10,time_masking=0.16)),\n","                            \n","                                  ],p=AudioConfig.spec_aug_prob),\n","                                  \n","            \n","            albumentations.Lambda(ImageStack(),p=1),\n","            albumentations.Lambda(ToColor(),p=1),\n","            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n","            albumentations.pytorch.transforms.ToTensor(),\n","            \n","            \n","        ])\n","    else:\n","        transforms = albumentations.Compose([\n","            albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=1),\n","            \n","            albumentations.CenterCrop(AudioConfig.n_mels,width,p=1),\n","            albumentations.Resize(height,width,p=1),\n","            albumentations.Lambda(ImageStack(),p=1),\n","            albumentations.Lambda(ToColor(),p=1),\n","            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n","            albumentations.pytorch.transforms.ToTensor(),\n","            \n","        ])\n","    return transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jziAsIAj5umN"},"source":["## Mixers"]},{"cell_type":"code","metadata":{"id":"pwNhBxJq5v0x"},"source":["def get_random_sample(dataset):\n","    rnd_idx = random.randint(0, len(dataset) - 1)\n","    rnd_audio,rnd_target=dataset.tensor_dict[df.loc[rnd_idx,dataset.path_col]]\n","    rnd_target=dataset.class_to_idx[rnd_target]\n","    \n","    rnd_audio = dataset.transform(image=rnd_audio[0])[\"image\"]\n","    rnd_target=dataset.target_transform(rnd_target,num_classes=len(dataset.classes))\n","    return rnd_audio, rnd_target\n","\n","class AddMixer:\n","    def __init__(self, alpha_dist='uniform'):\n","        assert alpha_dist in ['uniform', 'beta']\n","        self.alpha_dist = alpha_dist\n","\n","    def sample_alpha(self):\n","        if self.alpha_dist == 'uniform':\n","            return random.uniform(0, 0.5)\n","        elif self.alpha_dist == 'beta':\n","            return np.random.beta(0.4, 0.4)\n","\n","    def __call__(self, dataset, image, target):\n","        rnd_image, rnd_target = get_random_sample(dataset)\n","        alpha = self.sample_alpha()\n","        image = (1 - alpha) * image + alpha * rnd_image\n","        target = (1 - alpha) * target + alpha * rnd_target\n","        return image, target\n","\n","\n","class SigmoidConcatMixer:\n","    def __init__(self, sigmoid_range=(3, 12)):\n","        self.sigmoid_range = sigmoid_range\n","\n","    def sample_mask(self, size):\n","        x_radius = random.randint(*self.sigmoid_range)\n","\n","        step = (x_radius * 2) / size[1]\n","        x = np.arange(-x_radius, x_radius, step=step)\n","        y = torch.sigmoid(torch.from_numpy(x)).numpy()\n","        mix_mask = np.tile(y, (size[0], 1))\n","        return torch.from_numpy(mix_mask.astype(np.float32))\n","\n","    def __call__(self, dataset, image, target):\n","        rnd_image, rnd_target = get_random_sample(dataset)\n","\n","        mix_mask = self.sample_mask(image.shape[-2:])\n","        rnd_mix_mask = 1 - mix_mask\n","\n","        image = mix_mask * image + rnd_mix_mask * rnd_image\n","        target = target + rnd_target\n","        target = np.clip(target, 0.0, 1.0)\n","        return image, target\n","\n","\n","class RandomMixer:\n","    def __init__(self, mixers, p=None):\n","        self.mixers = mixers\n","        self.p = p\n","\n","    def __call__(self, dataset, image, target):\n","        mixer = np.random.choice(self.mixers, p=self.p)\n","        image, target = mixer(dataset, image, target)\n","        return image, target\n","\n","\n","class UseMixerWithProb:\n","    def __init__(self, mixer, prob=.5):\n","        self.mixer = mixer\n","        self.prob = prob\n","\n","    def __call__(self, dataset, image, target):\n","        if random.random() < self.prob:\n","            return self.mixer(dataset, image, target)\n","            print(image.shape,target.shape)\n","        return image, target\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DinrkdkYBhNf"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"d61H-5yvrh6f"},"source":["!cp \"/content/drive/My Drive/kaggle competitions/GIZ_NLP/audio_files.zip\" . >>/dev/null\n","!unzip audio_files.zip >>/dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyMWwGipcY0B"},"source":["!cp \"/content/drive/My Drive/kaggle competitions/GIZ_NLP/AdditionalUtterances.zip\" . >>/dev/null\n","!unzip AdditionalUtterances.zip >>/dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzD19fcJlWSJ"},"source":["!cp \"/content/drive/My Drive/kaggle competitions/GIZ_NLP/nlp_keywords_29Oct2020.zip\" . >>/dev/null\n","!unzip nlp_keywords_29Oct2020.zip >>/dev/null\n","!rm 'nlp_keywords/.DS_Store'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYmN4kE-JAyS"},"source":["df1=pd.read_csv(\"Train.csv\")\n","submission=pd.read_csv(\"SampleSubmission.csv\")\n","submission[\"label\"]=\"akawuka\"\n","df2,_=create_csv_dataset_from_path(Config.DATASET2_PATH)\n","df3,_=create_csv_dataset_from_path(Config.DATASET3_PATH)\n","df=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)\n","#df=df1\n","df_all=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36rXFfKSGR5_","executionInfo":{"elapsed":47176,"status":"ok","timestamp":1606852744588,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"862147d7-eca1-493f-f346-a35ce0930eec"},"source":["print(\"Files in the dataset : \",len(os.listdir(Config.DATASET_PATH)))\n","print(\"train1 shape : \",df1.shape)\n","print(\"train2 shape : \",df2.shape)\n","print(\"train3 shape : \",df3.shape)\n","print(\"train all shape : \",df.shape)\n","print(\"test shape : \",submission.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files in the dataset :  2126\n","train1 shape :  (1109, 2)\n","train2 shape :  (1740, 2)\n","train3 shape :  (1860, 2)\n","train all shape :  (4709, 2)\n","test shape :  (1017, 195)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lerCPU6BGZ4C"},"source":["classes=df[\"label\"].unique()\n","class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n","idx_to_class={idx:classe for idx,classe in enumerate(classes)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNOiHN30vu_G"},"source":["# Dataset definition and loading data"]},{"cell_type":"code","metadata":{"id":"dcBYAbEwLuVS"},"source":["class CSVDataset(Dataset):\n","    def __init__(self, df, loader,classes=None, transform=None,\n","                 target_transform=None,device=torch.device(\"cpu\")):\n","        super(Dataset, self).__init__()\n","        \n","        self.df=df.reset_index(drop=True)\n","        self.loader=loader\n","        self.transform=transform\n","        \n","        self.target_transform=target_transform\n","        self.device=device\n","        self.loaded=False\n","        self.loaded_samples=[]\n","        self.path_col=\"fn\"\n","        self.target_col=\"label\"\n","        if classes is None:\n","            self.classes=df[self.target_col].unique()\n","        else :\n","            self.classes=classes\n","        \n","        self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n","        self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n","        \n","    \n","    def load_data(self):\n","        self.loaded_samples=[]\n","        for ind in tqdm(range(len(self.df)),0):\n","            path=self.df.loc[ind,self.path_col]\n","            target=self.df.loc[ind,self.target_col]\n","            sample = self.loader(path)\n","            self.loaded_samples.append([sample,target])\n","        self.loaded=True\n","        \n","    def save_tensor(self,path):\n","        assert self.loaded==True\n","        torch.save(self.loaded_samples,path)\n","    \n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            dict {\"audio\" \"sample_rate\" \"target\"}\n","        \"\"\"\n","        if self.loaded:\n","            sample, target = self.loaded_samples[index]\n","        \n","        else:    \n","            path=self.df.loc[index,self.path_col]\n","            target=self.df.loc[index,self.target_col]\n","            sample = self.loader(path)\n","\n","        audio=sample[0]\n","        sample_rate=sample[1]\n","        target=self.class_to_idx[target]\n","        \n","        if self.transform is not None:\n","            audio = self.transform(image=audio)[\"image\"]\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        \n","        \n","\n","        \n","        return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n","    \n","    def __len__(self):\n","        return self.df.shape[0]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH4xyFDV_yKa"},"source":["class PathDataset(Dataset):\n","\n","    def __init__(self, df, tensor_dict,classes=None, transform=None,\n","                  target_transform=None,mixer=None,device=torch.device(\"cpu\")):\n","          super(PathDataset, self).__init__()\n","          \n","          self.df=df.reset_index(drop=True)\n","          self.tensor_dict=tensor_dict\n","          self.transform=transform\n","          self.mixer=mixer\n","          self.target_transform=target_transform\n","          self.device=device\n","          self.path_col=\"fn\"\n","          self.target_col=\"label\"\n","          if classes is None:\n","              self.classes=df[self.target_col].unique()\n","          else :\n","              self.classes=classes\n","          \n","          self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n","          self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n","\n","    def __getitem__(self, index):\n","          \"\"\"\n","          Args:\n","              index (int): Index\n","\n","          Returns:\n","              dict {\"audio\" \"sample_rate\" \"target\"}\n","          \"\"\"\n","          \n","          path=self.df.loc[index,self.path_col]\n","          \n","          sample,target = self.tensor_dict[path]\n","          audio=sample[0]\n","          sample_rate=sample[1]\n","          target=class_to_idx[target]\n","          \n","          if self.transform is not None:\n","              audio = self.transform(image=audio)[\"image\"]\n","\n","          if self.target_transform is not None:\n","              target = self.target_transform(target,num_classes=len(self.classes))\n","\n","          \n","          if self.mixer is not None:\n","              audio, target = self.mixer(self, audio, target)\n","\n","          \n","          return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n","\n","\n","    def __len__(self):\n","        return self.df.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoL2RSWjB1ho"},"source":["train_transform = get_transforms(train=True,height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ClD_PHK_63YZ"},"source":["amixer = RandomMixer([\n","        SigmoidConcatMixer(sigmoid_range=(3, 12)),\n","        AddMixer(alpha_dist='uniform')\n","    ], p=[0.6, 0.4])\n","amixer = UseMixerWithProb(amixer, prob=0.0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4l0T4PnPrKa"},"source":["audio_set=CSVDataset(df_all,read_as_melspectrogram,classes=classes,transform=train_transform)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hrha5HiNmwYI"},"source":["submission_set=CSVDataset(submission,read_as_melspectrogram,classes=classes,transform=train_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["b00d01a3eb2e494e8202509ffe21dab8","5ca4c7571d7d43f2991b5c6c55948c49","8fea14e1e8ae44de9b11496fc9c37e3a","447854096bc24952985b2a1199e19e75","16f4beb9416549a8831bf77249268469","7c8b3548cb91442b9c88673446d91b40","3debd91c77c24df3aabbfc1a8e642a2f","efff836f8be349dc8d0c288dc3375c3d"]},"id":"Z2-o95eNKVUZ","executionInfo":{"elapsed":2044819,"status":"ok","timestamp":1606854742267,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"07e08db2-d662-46a5-fe60-2b1dbccb5c2d"},"source":["audio_set.load_data()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b00d01a3eb2e494e8202509ffe21dab8","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4709.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VnzdXCN9B7NL"},"source":["tensor_dict={path:audio_set.loaded_samples[idx] for idx,path in enumerate(df_all[audio_set.path_col])}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzXOWW_0Cz-i"},"source":["loaded_set=PathDataset(df_all,tensor_dict,classes=classes,transform=train_transform,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jNUNeYcD7R-","executionInfo":{"elapsed":2044813,"status":"ok","timestamp":1606854742275,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"4eae967a-3310-420e-b623-cd6dfc104356"},"source":["loaded_set.__getitem__(457)[\"audio\"].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 400, 228])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["2df9646852ee449db8001e9e81cdd340","b8e991f74a1e4a6a9f8d56e3e4a0b71a","1b7fe1ef9764421f988004a9ae69df18","5a9b358f45554c28ac1c149311dd5dff","5dd6754dee66409da4e94042321ced10","b29c36f086c24633b78c08f43349fad2","fc7a8d94ec8a4ae7a6840c529d33d2f0","43fda713176a4d8ca7eb76222d03a456"]},"id":"JUnnVgyub9jb","executionInfo":{"elapsed":2500666,"status":"ok","timestamp":1606855198139,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"28130fd0-8dda-4517-a436-d12606054c9d"},"source":["submission_set.load_data()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2df9646852ee449db8001e9e81cdd340","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1017.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zw_moWkWcDdn"},"source":["submission_tensor_dict={path:submission_set.loaded_samples[idx] for idx,path in enumerate(submission[submission_set.path_col])}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apxsiVm2pad6"},"source":["# Class weights"]},{"cell_type":"code","metadata":{"id":"Hdpk0JI9sqbJ"},"source":["df1[\"num_label\"]=df1.label.apply(lambda x:class_to_idx[x])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yP4rQaujpZ0K","executionInfo":{"elapsed":2500662,"status":"ok","timestamp":1606855198148,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"06488d2f-2e84-4625-a835-fe15109afdd1"},"source":["class_weights = class_weight.compute_class_weight('balanced',\n","                                                 classes,\n","                                                 df1.label)\n","class_weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.5746114 , 1.4365285 , 0.63845711, 0.95768566, 1.1492228 ,\n","       0.82087343, 0.71826425, 0.95768566, 1.1492228 , 0.95768566,\n","       0.63845711, 0.47884283, 1.4365285 , 0.82087343, 0.71826425,\n","       1.4365285 , 0.63845711, 0.522374  , 1.1492228 , 1.91537133,\n","       0.522374  , 1.1492228 , 0.522374  , 0.82087343, 0.5746114 ,\n","       0.5746114 , 1.4365285 , 1.1492228 , 0.95768566, 0.95768566,\n","       0.95768566, 0.95768566, 1.91537133, 1.1492228 , 0.95768566,\n","       0.71826425, 0.82087343, 0.95768566, 0.82087343, 1.1492228 ,\n","       0.95768566, 0.82087343, 1.4365285 , 1.91537133, 0.82087343,\n","       1.4365285 , 0.82087343, 1.1492228 , 0.522374  , 1.4365285 ,\n","       0.95768566, 0.522374  , 0.95768566, 0.5746114 , 1.4365285 ,\n","       0.95768566, 1.4365285 , 0.71826425, 1.1492228 , 0.95768566,\n","       0.82087343, 1.1492228 , 1.4365285 , 1.1492228 , 0.63845711,\n","       0.82087343, 0.63845711, 1.1492228 , 1.4365285 , 0.522374  ,\n","       0.71826425, 0.95768566, 1.91537133, 1.1492228 , 1.1492228 ,\n","       1.91537133, 1.4365285 , 0.5746114 , 0.71826425, 0.63845711,\n","       1.1492228 , 1.91537133, 0.63845711, 0.95768566, 0.82087343,\n","       0.522374  , 1.1492228 , 0.95768566, 0.71826425, 1.91537133,\n","       1.1492228 , 1.1492228 , 1.4365285 , 0.82087343, 1.4365285 ,\n","       0.95768566, 1.1492228 , 1.4365285 , 0.63845711, 0.95768566,\n","       0.82087343, 1.4365285 , 0.63845711, 1.1492228 , 0.95768566,\n","       0.71826425, 0.95768566, 1.1492228 , 0.95768566, 1.1492228 ,\n","       1.1492228 , 1.1492228 , 1.4365285 , 1.1492228 , 0.5746114 ,\n","       0.82087343, 0.82087343, 0.95768566, 1.4365285 , 1.91537133,\n","       1.91537133, 0.82087343, 1.91537133, 1.91537133, 1.1492228 ,\n","       0.95768566, 0.82087343, 1.1492228 , 0.95768566, 1.4365285 ,\n","       1.4365285 , 1.4365285 , 1.4365285 , 1.1492228 , 1.91537133,\n","       0.95768566, 0.95768566, 0.71826425, 1.1492228 , 0.95768566,\n","       1.1492228 , 1.91537133, 0.95768566, 1.1492228 , 0.95768566,\n","       1.91537133, 1.1492228 , 1.4365285 , 0.82087343, 1.1492228 ,\n","       1.4365285 , 0.95768566, 1.1492228 , 0.82087343, 1.4365285 ,\n","       0.82087343, 1.91537133, 1.4365285 , 1.4365285 , 0.82087343,\n","       0.5746114 , 1.4365285 , 1.4365285 , 1.4365285 , 0.71826425,\n","       1.91537133, 1.4365285 , 1.1492228 , 0.82087343, 1.4365285 ,\n","       1.91537133, 0.82087343, 0.95768566, 1.4365285 , 1.4365285 ,\n","       1.1492228 , 1.4365285 , 0.95768566, 0.95768566, 1.1492228 ,\n","       1.4365285 , 1.91537133, 0.95768566, 0.95768566, 1.91537133,\n","       1.91537133, 1.4365285 , 1.91537133, 1.91537133, 1.4365285 ,\n","       1.91537133, 1.91537133, 1.91537133])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"slA-8YNAwplp"},"source":["# Folds"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqgyHUwk_DFS","executionInfo":{"elapsed":2500655,"status":"ok","timestamp":1606855198149,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"bccf6b43-2df3-4de9-8883-3103eeb349b3"},"source":["df[\"folds\"]=-1\n","df2[\"folds\"]=-1\n","df3[\"folds\"]=-1\n","\n","kf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.random_state, shuffle=True)\n","\n","\n","for fold, (_, val_index) in enumerate(kf.split(df,df[\"label\"])):\n","        df.loc[val_index, \"folds\"] = fold"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"jfuD3RXRig61"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"ppGrcXJEiiat"},"source":["class Net(torch.nn.Module):\n","    def __init__(self,arch,num_classes ,pretrained='imagenet'):\n","        super(Net, self).__init__()\n","        self.base_model = pretrainedmodels.__dict__[\n","            arch\n","        ](pretrained=pretrained)\n","        \n","        \n","        \n","        if arch==\"dpn98\":\n","            self.l0 = torch.nn.Linear(2688, num_classes)\n","        elif arch==\"se_resnext50_32x4d\" or arch==\"resnet101\" :\n","            self.l0 = torch.nn.Linear(2048, num_classes)\n","        elif arch==\"dpn68\":\n","            self.l0 = torch.nn.Linear(832, num_classes)\n","        elif arch==\"resnet18\":\n","            self.l0 = torch.nn.Linear(512, num_classes)\n","            \n","\n","        elif arch==\"vgg19\":\n","            self.l0 = torch.nn.Linear(512, num_classes)\n","            \n","\n","        elif arch==\"se_resnet50\":    \n","            self.l0 = torch.nn.Linear(2048, num_classes)\n","        elif arch==\"resnet50\":    \n","            self.l0 = torch.nn.Linear(2048, num_classes)  \n","        elif arch==\"senet154\":\n","            self.l0 = torch.nn.Linear(2048, num_classes)\n","        elif arch==\"se_resnext101_32x4d\":\n","            self.l0 = torch.nn.Linear(2048, num_classes)\n","        elif arch==\"dpn107\":\n","            self.l0 = torch.nn.Linear(2688, num_classes)\n","        elif arch==\"densenet121\":\n","            self.l0 = torch.nn.Linear(1024, num_classes)\n","            fc_size = self.base_model.last_linear.in_features\n","            \n","            self.base_model.last_linear = nn.Sequential(nn.Linear(7168, 193))\n","\n","\n","        else :\n","            self.l0 = torch.nn.Linear(4098, num_classes)\n","    def forward(self, audio, target,sample_rate):\n","        batch_size, _, _, _ = audio.shape\n","        x=audio\n","        x = self.base_model.features(x)\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n","        out = self.l0(x)\n","        loss = torch.nn.CrossEntropyLoss()(out, torch.argmax(target, dim=1))\n","        return out, loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hu_LCWchgVr"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"0Pnh8tcz5gVp"},"source":["model_name=\"resnet18\"\n","pretrained=\"imagenet\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["12618002b54146b485722a50d5433e91","e89e4a71427143fc807dfea473b5951d","8d6ab6e0f5a14af7ada8d7761bdb3deb","635acd7387714fe4bf600a3d100bbdee","f00c41f347254fb2b8408afe9bd18d8f","467178f4de654ad1a7467011c315c699","48a28b28baa5408ca74dbfe568658379","af94b0bec4764ba7b8ed0babd00dbb4e"]},"id":"QkfdfenPC1kS","executionInfo":{"elapsed":2500979,"status":"ok","timestamp":1606855198491,"user":{"displayName":"Mokhtar Mami","photoUrl":"","userId":"17489496427386187561"},"user_tz":-60},"outputId":"4bef6d2c-47b6-448b-b4d0-67bb925d2927"},"source":["model = Net(model_name,num_classes=len(classes),pretrained=pretrained)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12618002b54146b485722a50d5433e91","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r5XRMa6M1WKr"},"source":["def train(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n","    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n","    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n","    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    valid_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    mixer = RandomMixer([\n","        AddMixer(alpha_dist='uniform')\n","    ], p=[1])\n","    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n","    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n","    \n","    \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n","    )\n","    \n","    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n","    \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    \n","    model.to(Config.device)\n","    model.train()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n","    \n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, factor=0.6,min_lr=Config.min_lr,verbose=True)\n","    \n","\n","    es = EarlyStopping(patience=8, mode=\"min\")\n","    eng = Engine(model, optimizer, device=Config.device)\n","    for epoch in range(Config.epochs):\n","        train_loss = eng.train(train_loader)\n","        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n","        with open('out.txt', 'a') as f:\n","            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n","        \n","        scheduler.step(valid_loss)\n","        es(valid_loss, model, model_path=f\"model_fold_{fold}.bin\")\n","        if es.early_stop:\n","            print(\"Early stopping\")\n","            break\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tminX_NX_hia"},"source":["def after_train(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    model_save_path=f\"model_fold_{fold}.bin\"\n","    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n","    \n","    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n","    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n","    \n","    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    valid_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    mixer = RandomMixer([\n","        \n","        AddMixer(alpha_dist='uniform')\n","    ], p=[1])\n","    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n","    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n","    \n","    \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n","    )\n","    \n","    \n","    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n","    \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(Config.device)\n","    model.train()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.aftertrain_lr)\n","    \n","    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0.2*1e-6, last_epoch=-1,)\n","    \n","    es = EarlyStopping(patience=6, mode=\"min\")\n","    eng = Engine(model, optimizer,scheduler=scheduler, device=Config.device)\n","    for epoch in range(Config.epochs):\n","        if epoch!=0:\n","            train_loss = eng.train(train_loader)\n","        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n","        \n","        with open('out.txt', 'a') as f:\n","            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n","        \n","        \n","        es(valid_loss, model, model_path=model_save_path)\n","        if es.early_stop:\n","            print(\"Early stopping\")\n","            break\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgvg-XQQEf2_"},"source":["def eval_train(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n","    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n","    \n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    valid_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    mixer = RandomMixer([\n","        \n","        AddMixer(alpha_dist='uniform')\n","    ], p=[1])\n","    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n","    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n","    \n","    \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n","    )\n","\n","    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n","    \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(Config.device)\n","    model.train()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n","    eng = Engine(model, optimizer, device=Config.device)\n","    \n","    train_loss,predictions = eng.evaluate(train_loader, return_predictions=True)\n","    valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n","    \n","    \n","    print(f\"train loss = {train_loss}, valid loss = {valid_loss} \")\n","    return train_loss,valid_loss\n","\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7fZjwth7AYZ"},"source":["def predict(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    test_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    \n","    \n","    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=test_transfrom,target_transform=to_categorical)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(Config.device)\n","    model.eval()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr)\n","    eng = Engine(model, optimizer, device=Config.device)\n","    predictions = eng.predict(test_loader)\n","    #valid_loss,predictions = eng.evaluate(test_loader, return_predictions=True)\n","    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWQPt_MTu5OA"},"source":["def predict_tta(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    test_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    \n","    \n","    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical)\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(Config.device)\n","    model.eval()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n","    eng = Engine(model, optimizer, device=Config.device)\n","    all_predictions=[]\n","    for i in range(30):\n","        all_predictions.append(torch.nn.Softmax(dim=1)(torch.cat(eng.predict(test_loader))).numpy())\n","    \n","    predictions=gmean(all_predictions)\n","    \n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EudD6O6ILrfi"},"source":["def generate_submission_csv(fold):\n","    seed_all(num_seed)\n","    model_path=f\"model_fold_{fold}.bin\"\n","    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n","    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n","    \n","    train_transfrom = get_transforms(train=True,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    valid_transfrom = get_transforms(train=False,\n","                                     height=AudioConfig.height,\n","                                     width=AudioConfig.width,\n","                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n","    mixer = RandomMixer([\n","        \n","        AddMixer(alpha_dist='uniform')\n","    ], p=[1])\n","    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n","    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n","    \n","    \n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n","    )\n","\n","    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n","    \n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n","    )\n","    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(Config.device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n","    eng = Engine(model, optimizer, device=Config.device)\n","    \n","    \n","    predictions = eng.predict(valid_loader)\n","    \n","    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n","    sample = df[df[\"folds\"] == fold].reset_index(drop=True)\n","    \n","    sample.loc[:, classes] = predictions\n","    return sample\n","\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgmL6qprRPS9"},"source":["11:54"]},{"cell_type":"markdown","metadata":{"id":"0XOGbYimYlsb"},"source":["### Training folds"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wEDczcmboOhY","outputId":"6d74198d-e542-4508-af46-0c46c896bca3"},"source":["%%capture\n","for fold in range(0,Config.n_folds):\n","    print(\"Fold : \",fold)\n","    train(fold)\n","    after_train(fold)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"referenced_widgets":["236a3324bce647d897e5659db1678129","5bdd4023c20b4e7e8e66ab4972bf3d12","a4a2c48693b14578b93a4c39907779b1","8e52249a2e74430992f35afab81490e5","1a5457c170f7435c98dfc9aa5193eab2","ae3b1891461442acb10d3369ec452c5e","c2de172ec7c245e381fa6b35cb334c1c","87295100c4e147f986f6535ed8212daf","863ae1cfae4f485e83a5b16a44516926","a52b97b362bd4a9085a2726ecabefea5","be3f64cfbd7248609ff6c90b0de77f1f","9db9a5f0591b421484525904371faea7","054aedb3657a47ee8d699016b6d0cd7a","ebce19cf14f04530b65428bfa34359a8","cea3ed91f49c42c68250fe4e9636e80b","b357d096812b4f4da8aee702db77b440","48ef8127057e43278b2a1a84a31e1fb4","23fd6435ff114798882a2cf38017beb9","1f634377d7734acb98b3fe1c523ab0d9","f23414e3d23a4e0583496fb860b0025a"]},"id":"dM-Vv3sFFhvw","outputId":"fd38e913-d9d0-46ee-cf67-1503052b6b00"},"source":["train_losses=[]\n","valid_losses=[]\n","for fold in range(Config.n_folds):\n","    train_loss,valid_loss=eval_train(fold)\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"236a3324bce647d897e5659db1678129","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bdd4023c20b4e7e8e66ab4972bf3d12","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.09495743758353747, valid loss = 0.6202142954526777 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4a2c48693b14578b93a4c39907779b1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e52249a2e74430992f35afab81490e5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.09659368704493008, valid loss = 0.3911043093578004 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a5457c170f7435c98dfc9aa5193eab2","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae3b1891461442acb10d3369ec452c5e","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.0897328405242695, valid loss = 0.49899759575289526 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2de172ec7c245e381fa6b35cb334c1c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87295100c4e147f986f6535ed8212daf","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.06754315910235026, valid loss = 0.3659040279443755 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"863ae1cfae4f485e83a5b16a44516926","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a52b97b362bd4a9085a2726ecabefea5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.0876363061122284, valid loss = 0.3790180476851489 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be3f64cfbd7248609ff6c90b0de77f1f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9db9a5f0591b421484525904371faea7","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.323479715663004, valid loss = 0.66794606851329 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"054aedb3657a47ee8d699016b6d0cd7a","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebce19cf14f04530b65428bfa34359a8","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.07369889755984058, valid loss = 0.5461316176194903 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cea3ed91f49c42c68250fe4e9636e80b","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b357d096812b4f4da8aee702db77b440","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.08898564089229798, valid loss = 0.4097290407882025 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48ef8127057e43278b2a1a84a31e1fb4","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23fd6435ff114798882a2cf38017beb9","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.150060227658249, valid loss = 0.645256671358997 \n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f634377d7734acb98b3fe1c523ab0d9","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1060.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f23414e3d23a4e0583496fb860b0025a","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=118.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train loss = 0.08962246341693829, valid loss = 0.49708985211919615 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"VZ6124k4CPsH","outputId":"89dde911-3823-49c6-e263-034086fe26c8"},"source":["np.mean(valid_losses)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5021391526592074"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"N7aK8tRv1bj0"},"source":["p=[]\n","for fold in range(Config.n_folds):\n","    p.append(predict(fold).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"693biZU0ptws"},"source":["predictions=gmean(p,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"sxD_ihkLgCCg","outputId":"4e1c1eba-0d57-4a54-d14b-c4acdb194df9"},"source":["predictions.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1017, 193)"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"aZ-fHW9EIC0M"},"source":["\n","prediction_file=f\"resnet18_pcen.csv\"\n","sample = pd.read_csv(\"SampleSubmission.csv\")\n","sample.loc[:, classes] = predictions\n","sample.to_csv(prediction_file, index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TE1O2f9yL5cW"},"source":[""],"execution_count":null,"outputs":[]}]}