{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dpn68_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaSBI1FrBPbO"
      },
      "source": [
        "# Drive and env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4-rDTVFBRs-"
      },
      "source": [
        "%pip install git+https://github.com/Mo5mami/wtfml.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0vy-CXECUEx"
      },
      "source": [
        "%pip install torchaudio librosa pretrainedmodels albumentations==0.4.6 imblearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAavaqBYGuXU",
        "outputId": "5727894b-b21c-46cc-eaa6-22f566da19fb"
      },
      "source": [
        "!pip freeze | grep torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch==1.7.0\r\n",
            "torchaudio==0.7.0a0+ac17b64\r\n",
            "torchtext==0.8.0a0+cd6902d\r\n",
            "torchvision==0.8.1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldjVWLvw-to9",
        "outputId": "bf9c9603-9c22-423a-a81f-78f00f597677"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import librosa\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "from scipy.io import wavfile\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "#from utils import one_hot_embedding\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit\n",
        "import albumentations\n",
        "\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "\n",
        "from wtfml.utils import EarlyStopping\n",
        "from wtfml.engine import Engine\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels.models import nasnetamobile\n",
        "import cv2\n",
        "import gc\n",
        "import math\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats.mstats import gmean\n",
        "from librosa.display import specshow\n",
        "from sklearn.utils import class_weight\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import io\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
            "  '\"sox\" backend is being deprecated. '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Dpvo2EIs2-"
      },
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "num_seed=42\n",
        "seed_all(num_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x81-1ToVpEMh",
        "outputId": "478b8aa5-37e9-4f64-c80d-a8f31d19e5f5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 23 13:05:25 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla M60           On   | 000068DF:00:00.0 Off |                  Off |\n",
            "| N/A   32C    P8    14W / 150W |      3MiB /  8129MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDZbdWcfGZA7",
        "outputId": "5192d3a5-a588-481f-a082-371b01b02b4e"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6hTOg7_BlR6"
      },
      "source": [
        "# Utils and settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up4hXqtdi4mE"
      },
      "source": [
        "## general settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owtw_hGLIojb"
      },
      "source": [
        "class Config:\n",
        "  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  epochs=40\n",
        "  random_state=42\n",
        "  train_batchsize=4\n",
        "  test_batchsize=4\n",
        "  val_every=5\n",
        "  print_every=20\n",
        "  logdir=\"logs\"\n",
        "  DATASET_PATH=\"audio_files\"\n",
        "  DATASET2_PATH=\"latest_keywords\"\n",
        "  DATASET3_PATH=\"nlp_keywords\"\n",
        "  n_folds=10\n",
        "  test_size=0.1\n",
        "  lr=1.2*1e-4\n",
        "  aftertrain_lr=2*1e-6\n",
        "  min_lr=0.1*1e-4\n",
        "  experiment_id=\"models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4RHFE9d9N_a"
      },
      "source": [
        "os.mkdir(Config.experiment_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYM0FspMi6Po"
      },
      "source": [
        "## audio settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcWj0gfA1FS4"
      },
      "source": [
        "class AudioConfig:\n",
        "    audio_length=3\n",
        "    sr=44100\n",
        "    #sr=44100\n",
        "    fixed_sr=audio_length*sr\n",
        "    #hop_length = 512\n",
        "    #hop_length = 275\n",
        "    hop_length = 276\n",
        "    fmin = 20\n",
        "    fmax = 8000\n",
        "    n_mels = 64\n",
        "    n_mfcc=13\n",
        "    #n_fft = 8192\n",
        "    n_fft = n_mels*20\n",
        "    #n_fft=8000\n",
        "    min_seconds = 0.1\n",
        "    #CROP_SIZE = 247\n",
        "    WRAP_PAD_PROB = 0.5\n",
        "    pad=400\n",
        "    spec_aug_prob=0.8\n",
        "    mixer_prob=0.0\n",
        "    audio_crop_prob=0.5\n",
        "    height=228\n",
        "    width=400\n",
        "    duration=3.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofe5D8qVC8B1"
      },
      "source": [
        "## plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSw0i21SCRQ5"
      },
      "source": [
        "def plot_signal(signals):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n",
        "                             sharey=True, figsize=(20,5))\n",
        "    axes.set_title(\"sig\")\n",
        "    axes.plot(list(signals))\n",
        "    \n",
        "    \n",
        "def plot_signals(signals):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
        "                             sharey=True, figsize=(20,5))\n",
        "    fig.suptitle('Time Series', size=16)\n",
        "    i = 0\n",
        "    for x in range(2):\n",
        "        for y in range(5):\n",
        "            axes[x,y].set_title(list(signals.keys())[i])\n",
        "            axes[x,y].plot(list(signals.values())[i])\n",
        "            axes[x,y].get_xaxis().set_visible(False)\n",
        "            axes[x,y].get_yaxis().set_visible(False)\n",
        "            i += 1\n",
        "\n",
        "def plot_fft(fft):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
        "                             sharey=True, figsize=(20,5))\n",
        "    fig.suptitle('Fourier Transforms', size=16)\n",
        "    i = 0\n",
        "    for x in range(2):\n",
        "        for y in range(5):\n",
        "            data = list(fft.values())[i]\n",
        "            Y, freq = data[0], data[1]\n",
        "            axes[x,y].set_title(list(fft.keys())[i])\n",
        "            axes[x,y].plot(freq, Y)\n",
        "            axes[x,y].get_xaxis().set_visible(False)\n",
        "            axes[x,y].get_yaxis().set_visible(False)\n",
        "            i += 1\n",
        "\n",
        "def plot_fbank(fbank):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
        "                             sharey=True, figsize=(20,5))\n",
        "    fig.suptitle('Filter Bank Coefficients', size=16)\n",
        "    i = 0\n",
        "    for x in range(2):\n",
        "        for y in range(5):\n",
        "            axes[x,y].set_title(list(fbank.keys())[i])\n",
        "            axes[x,y].imshow(list(fbank.values())[i],\n",
        "                    cmap='hot', interpolation='nearest')\n",
        "            axes[x,y].get_xaxis().set_visible(False)\n",
        "            axes[x,y].get_yaxis().set_visible(False)\n",
        "            i += 1\n",
        "\n",
        "def plot_mfccs(mfccs):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n",
        "                             sharey=True, figsize=(20,5))\n",
        "    \n",
        "    axes.set_title(\"mfcc\")\n",
        "    \n",
        "    specshow(mfccs,x_axis='time',y_axis='mel', \n",
        "                             sr=AudioConfig.sr, hop_length=AudioConfig.hop_length,\n",
        "                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    \n",
        "    plt.show()\n",
        "def get_plot_mfccs(mfccs):\n",
        "    \n",
        "    \n",
        "    fig,ax = plt.subplots(1)\n",
        "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    \n",
        "    specshow(mfccs,x_axis=\"time\",y_axis=\"mel\", \n",
        "                             sr=AudioConfig.sr,hop_length=AudioConfig.hop_length,\n",
        "                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    image=io.BytesIO()\n",
        "    fig.savefig(image,bbox_inches='tight',pad_inches=0.0)\n",
        "    img=np.frombuffer(image.getvalue(), dtype='uint8')\n",
        "    img = cv2.imdecode(img,cv2.IMREAD_COLOR)\n",
        "    image.close()\n",
        "    plt.close()\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "def plot_class_dist(X):\n",
        "    class_dis=X.groupby(\"label\")[\"fn\"].count()\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    ax.set_title('Class Distribution', y=1.08)\n",
        "    ax.pie(class_dis, labels=class_dis.index, autopct='%1.1f%%',\n",
        "          shadow=False, startangle=90)\n",
        "    ax.axis('equal')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_melspectrogram(mels, title='Log-frequency power spectrogram'):\n",
        "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
        "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
        "                            fmin=conf.fmin, fmax=conf.fmax)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc_Ouje2gWOW"
      },
      "source": [
        "## util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXrn6BrlB4tb"
      },
      "source": [
        "\"\"\"\n",
        "Read audio from a path\n",
        "\"\"\"\n",
        "\n",
        "def read_wav(filepath):\n",
        "  \n",
        "  sample_rate, samples = wavfile.read(filepath)\n",
        "  return sample_rate,np.array(samples)\n",
        "\n",
        "\"\"\"\n",
        "Listen to audio sample\n",
        "\"\"\"\n",
        "def listen(samples,sample_rate):\n",
        "  return ipd.Audio(samples, rate=sample_rate)\n",
        "\n",
        "\"\"\"\n",
        "Wav loader for DatasetFolder using librosa\n",
        "\"\"\"\n",
        "def wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n",
        "    sample,sr=librosa.load(path,sr=sr)\n",
        "    result=torch.zeros(1,fixed_sr)\n",
        "    length=min(fixed_sr,len(sample))\n",
        "    result[0,:length]=torch.tensor(sample[:length])\n",
        "    return (result,sr)\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "Wav loader for DatasetFolder using torchaudio\n",
        "\"\"\"\n",
        "\n",
        "def torch_wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n",
        "    sample,sr=torchaudio.load_wav(path)\n",
        "    result=torch.zeros(1,fixed_sr)\n",
        "    length=min(fixed_sr,sample.shape[1])\n",
        "    result[0,:length]=sample[0,:length]\n",
        "    return (result,sr)\n",
        "\n",
        "\"\"\"\n",
        "accuracy measure\n",
        "\"\"\"\n",
        "def accuracy(predictions,real):\n",
        "    return (predictions==real).sum()*100/len(predictions)\n",
        "\n",
        "def enveloppe(sig,sr,threshhold):\n",
        "  mask=[]\n",
        "  sig=pd.Series(sig).apply(np.abs)\n",
        "  sig_mean=sig.rolling(window=int(sr/10),min_periods=1,center=True).mean()\n",
        "  for mean in sig_mean:\n",
        "    if mean>threshhold:\n",
        "      mask.append(True)\n",
        "    else:  mask.append(False)\n",
        "  return mask\n",
        "\n",
        "\n",
        "def read_audio(file_path,top_db=60):\n",
        "    min_samples = int(AudioConfig.min_seconds * AudioConfig.sr)\n",
        "    y, sr = librosa.load(file_path, sr=AudioConfig.sr)\n",
        "    \n",
        "    trim_y, trim_idx = librosa.effects.trim(y,top_db=top_db,frame_length=AudioConfig.n_fft, hop_length=AudioConfig.hop_length)  # trim, top_db=default(60)\n",
        "\n",
        "    if len(trim_y) < min_samples:\n",
        "        center = (trim_idx[1] - trim_idx[0]) // 2\n",
        "        left_idx = max(0, center - min_samples // 2)\n",
        "        right_idx = min(len(y), center + min_samples // 2)\n",
        "        trim_y = y[left_idx:right_idx]\n",
        "\n",
        "        if len(trim_y) < min_samples:\n",
        "            padding = min_samples - len(trim_y)\n",
        "            offset = padding // 2\n",
        "            trim_y = np.pad(trim_y, (offset, padding - offset), 'constant')\n",
        "\n",
        "    \n",
        "    return trim_y\n",
        "\n",
        "def test_top_db(filepath,top_db,print_mask=True):\n",
        "  sample=read_audio(filepath)\n",
        "  plot_signal(sample)\n",
        "  sample_test=read_audio(filepath,top_db=top_db)\n",
        "  plot_signal(sample_test)\n",
        "  if(print_mask):  return listen(sample_test,AudioConfig.sr)\n",
        "  else : return listen(sample,AudioConfig.sr)\n",
        "\n",
        "\n",
        "def audio_to_melspectrogram(audio):\n",
        "    \n",
        "    spectrogram = librosa.feature.melspectrogram(audio,\n",
        "                                                 sr=AudioConfig.sr,\n",
        "                                                 n_mels=AudioConfig.n_mels,\n",
        "                                                 n_fft=AudioConfig.n_fft,\n",
        "                                                 hop_length=AudioConfig.hop_length,\n",
        "                                                 fmin=AudioConfig.fmin,\n",
        "                                                 fmax=AudioConfig.fmax,\n",
        "                                                 power=2\n",
        "                                                 )\n",
        "    spectrogram = librosa.power_to_db(spectrogram,ref=np.max)\n",
        "    spectrogram = spectrogram.astype(np.float32)\n",
        "    return spectrogram\n",
        "\n",
        "def read_as_melspectrogram(file_path,time_stretch=1.0, pitch_shift=0.0,\n",
        "                           debug_display=False):\n",
        "    x = read_audio(file_path)\n",
        "    if time_stretch != 1.0:\n",
        "        x = librosa.effects.time_stretch(x, time_stretch)\n",
        "\n",
        "    if pitch_shift != 0.0:\n",
        "        librosa.effects.pitch_shift(x, config.sampling_rate, n_steps=pitch_shift)\n",
        "\n",
        "    mels = audio_to_melspectrogram(x)\n",
        "    if debug_display:\n",
        "        import IPython\n",
        "        IPython.display.display(IPython.display.Audio(x, rate=config.sampling_rate))\n",
        "        show_melspectrogram(mels)\n",
        "    return (mels,AudioConfig.sr)\n",
        "\n",
        "def mix_up(x, y):\n",
        "        x = np.array(x, np.float32)\n",
        "        lam = np.random.beta(1.0, 1.0)\n",
        "        ori_index = np.arange(int(len(x)))\n",
        "        index_array = np.arange(int(len(x)))\n",
        "        np.random.shuffle(index_array)        \n",
        "        \n",
        "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
        "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
        "        \n",
        "        return mixed_x, mixed_y\n",
        "\n",
        "def oversample(dataframe):\n",
        "    X,y=RandomOverSampler(random_state=42).fit_sample(dataframe, dataframe[\"label\"])\n",
        "    return pd.DataFrame(X,columns=dataframe.columns).reset_index(drop=True)\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return torch.eye(num_classes, dtype=float)[y]\n",
        "\n",
        "\"\"\"\n",
        "function to test the enveloppe\n",
        "\"\"\"\n",
        "def test_enveloppe(filepath,thresh=0.0005,print_mask=True):\n",
        "  sample=read_audio(filepath)\n",
        "  print(sample.max())\n",
        "  plot_signal(sample)\n",
        "  mask=enveloppe(sample,AudioConfig.sr,thresh)\n",
        "  plot_signal(sample[mask])\n",
        "  if(print_mask):  return listen(sample[mask],AudioConfig.sr)\n",
        "  else : return listen(sample,AudioConfig.sr)\n",
        "\n",
        "def create_csv_dataset_from_path(dataset_path):\n",
        "    classes=[classe for classe in os.listdir(dataset_path)]\n",
        "    class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n",
        "    idx_to_class={idx:classe for idx,classe in enumerate(classes)}\n",
        "    \n",
        "    path=[]\n",
        "    target=[]\n",
        "    for classe in classes:\n",
        "        class_path=os.path.join(dataset_path,classe)\n",
        "        for sample in os.listdir(class_path):\n",
        "            path.append(os.path.join(class_path,sample))\n",
        "            target.append(classe)\n",
        "    \n",
        "    dataset=pd.DataFrame(data={\"fn\":path,\"label\":target})\n",
        "    dataset = dataset.sample(frac=1,random_state=42).reset_index(drop=True)\n",
        "    return dataset,classes\n",
        "\n",
        "def onset_test(path):\n",
        "    y=read_audio(path)\n",
        "    times = librosa.times_like(audio_to_melspectrogram(y))\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=AudioConfig.sr,\n",
        "                                         aggregate=np.median,\n",
        "                                         n_fft=AudioConfig.n_fft,\n",
        "                                        hop_length=AudioConfig.hop_length,\n",
        "                                         fmax=8000, n_mels=160)\n",
        "    print(onset_env.argmax())\n",
        "    print(onset_env.shape)\n",
        "    plt.plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,\n",
        "           label='Median (custom mel)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "672QOn7droRK"
      },
      "source": [
        "## Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6suJsK7f8Yk"
      },
      "source": [
        "### wav trans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywfq9BNegSIX"
      },
      "source": [
        "class ChangeAmplitude(object):\n",
        "    \"\"\"Changes amplitude of an audio randomly.\"\"\"\n",
        "\n",
        "    def __init__(self, amplitude_range=(0.7, 1.1)):\n",
        "        self.amplitude_range = amplitude_range\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        \n",
        "\n",
        "        image = image * random.uniform(*self.amplitude_range)\n",
        "        return data\n",
        "\n",
        "class ChangeSpeedAndPitchAudio(object):\n",
        "    \"\"\"Change the speed of an audio. This transform also changes the pitch of the audio.\"\"\"\n",
        "\n",
        "    def __init__(self, max_scale=0.2):\n",
        "        self.max_scale = max_scale\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        \n",
        "\n",
        "        samples = image\n",
        "        sample_rate = AudioConfig.sr\n",
        "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
        "        speed_fac = 1.0  / (1 + scale)\n",
        "        image = np.interp(np.arange(0, len(samples), speed_fac), np.arange(0,len(samples)), samples).astype(np.float32)\n",
        "        return image\n",
        "\n",
        "class StretchAudio(object):\n",
        "    \"\"\"Stretches an audio randomly.\"\"\"\n",
        "\n",
        "    def __init__(self, max_scale=0.2):\n",
        "        self.max_scale = max_scale\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        \n",
        "\n",
        "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
        "        image = librosa.effects.time_stretch(image, 1+scale)\n",
        "        return image\n",
        "class TimeshiftAudio(object):\n",
        "    \"\"\"Shifts an audio randomly.\"\"\"\n",
        "\n",
        "    def __init__(self, max_shift_seconds=0.2):\n",
        "        self.max_shift_seconds = max_shift_seconds\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "\n",
        "        samples = image\n",
        "        sample_rate = AudioConfig.sr\n",
        "        max_shift = (sample_rate * self.max_shift_seconds)\n",
        "        shift = random.randint(-max_shift, max_shift)\n",
        "        a = -min(0, shift)\n",
        "        b = max(0, shift)\n",
        "        samples = np.pad(samples, (a, b), \"constant\")\n",
        "        image = samples[:len(samples) - a] if a else samples[b:]\n",
        "        return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aTvelHZgXmJ"
      },
      "source": [
        "def gauss_noise(k,sig):\n",
        "     return np.random.normal(scale=k*np.max(sig), size=len(sig))\n",
        "\n",
        "def gn(samples,k=2e-2):\n",
        "    noise_g = gauss_noise(k,samples)\n",
        "    return samples+noise_g\n",
        "\n",
        "class GN(object):\n",
        "    \"\"\"Adds a random background noise.\"\"\"\n",
        "    def __init__(self, ):\n",
        "        None\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        k=np.random.uniform(low=8e-3, high=4e-2, size=None)\n",
        "        sample=self.gn(image,k=k)\n",
        "        return sample\n",
        "\n",
        "\"\"\"\n",
        "testing gaussian noise\n",
        "\"\"\"\n",
        "def test_transform(filepath,k=2e-2,print_mask=False):\n",
        "  sr=AudioConfig.sr\n",
        "  sample=read_audio(filepath)\n",
        "  plot_signal(sample)\n",
        "  \n",
        "  sample2=gn(sample,k=k)\n",
        "  plot_signal(sample2)\n",
        "  if(print_mask):  return listen(sample2,sr)\n",
        "  else : return listen(sample,sr)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gh-yj2Xf-x3"
      },
      "source": [
        "### spect transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3XhMhrcg0D0"
      },
      "source": [
        "class ToMfcc(object):\n",
        "    def __call__(self,image,**kwargs):\n",
        "        return audio_to_melspectrogram(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssDjVP1RyStS"
      },
      "source": [
        "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6,**kwargs):\n",
        "    X=X.transpose(1, 0, 2)\n",
        "    mean = mean or X.mean()\n",
        "    std = std or X.std()\n",
        "    Xstd = (X - mean) / (std + eps)\n",
        "    _min, _max = Xstd.min(), Xstd.max()\n",
        "    norm_max = norm_max or _max\n",
        "    norm_min = norm_min or _min\n",
        "    if (_max - _min) > eps:\n",
        "        # Normalize to [0, 255]\n",
        "        V = Xstd\n",
        "        V[V < norm_min] = norm_min\n",
        "        V[V > norm_max] = norm_max\n",
        "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
        "        V = V.astype(np.uint8)\n",
        "    else:\n",
        "        # Just zero\n",
        "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
        "    return V\n",
        "  \n",
        "\n",
        "class ToColor:\n",
        "    def __init__(self,\n",
        "                  mean=None,\n",
        "                  std=None):\n",
        "        self.mean=mean\n",
        "        self.std = std\n",
        "        \n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        return mono_to_color(image,\n",
        "                            self.mean,\n",
        "                            self.std,\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uf34VjlyUYW"
      },
      "source": [
        "class AudioCrop:\n",
        "    def __init__(self,percentage=0.75):\n",
        "        self.percentage=percentage\n",
        "\n",
        "    def __call__(self,image,**kwargs):\n",
        "        perc=np.random.random()*(1-self.percentage)+self.percentage\n",
        "        return albumentations.RandomCrop(image.shape[0],int(image.shape[1]*perc),p=1)(image=image)[\"image\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl8-3gOxod7c"
      },
      "source": [
        "class Onset:\n",
        "    def __init__(self,size):\n",
        "        self.size=size\n",
        "\n",
        "    def __call__(self,image,**kwargs):\n",
        "        onset_env = librosa.onset.onset_strength(S=image)\n",
        "        argmax=onset_env.argmax()\n",
        "        return albumentations.Crop(x_min=argmax-self.size//2, y_min=0, x_max=argmax+self.size//2, y_max=AudioConfig.n_mels,p=1)(image=image)[\"image\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz4KGA4aNIt4"
      },
      "source": [
        "class PadToSize:\n",
        "    def __init__(self, size, mode='constant'):\n",
        "        #assert mode in ['constant', 'wrap']\n",
        "        self.size = size\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        if image.shape[1] < self.size:\n",
        "            padding = self.size - image.shape[1]\n",
        "            offset = padding // 2\n",
        "            pad_width = ((0, 0), (offset, padding - offset))\n",
        "            #pad_width = ((0, 0), (0, padding ))\n",
        "            if self.mode == 'constant':\n",
        "                \n",
        "                #image = np.pad(image, pad_width,'constant', constant_values=image.min())\n",
        "                image = np.pad(image, pad_width,'constant', constant_values=0)\n",
        "            else:\n",
        "                image = np.pad(image, pad_width, 'wrap')\n",
        "        return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h85CAKDO8Jrx"
      },
      "source": [
        "class AudioPad:\n",
        "    def __init__(self,percentage=0.10, mode='constant'):\n",
        "        self.percentage=percentage\n",
        "        self.mode=mode\n",
        "    def __call__(self,image,**kwargs):\n",
        "        return PadToSize(int(image.shape[1]*(self.percentage+1)),self.mode)(image=image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFK_xL-vPr3x"
      },
      "source": [
        "class ImageStack:\n",
        "    def __call__(self, image,**kwargs):\n",
        "        delta = librosa.feature.delta(image)\n",
        "        accelerate = librosa.feature.delta(image, order=2)\n",
        "        image = np.stack([image, delta, accelerate], axis=-1)\n",
        "        image = image.astype(np.float32)\n",
        "        return image\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G99PGSMw_A3J"
      },
      "source": [
        "def spec_augment(spec: np.ndarray,\n",
        "                 num_mask=2,\n",
        "                 freq_masking=0.15,\n",
        "                 time_masking=0.20,\n",
        "                 value=0):\n",
        "    spec = spec.copy()\n",
        "    num_mask = random.randint(1, num_mask)\n",
        "    for i in range(num_mask):\n",
        "        all_freqs_num, all_frames_num  = spec.shape\n",
        "        freq_percentage = random.uniform(0.0, freq_masking)\n",
        "\n",
        "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
        "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
        "        f0 = int(f0)\n",
        "        spec[f0:f0 + num_freqs_to_mask, :] = value\n",
        "\n",
        "        time_percentage = random.uniform(0.0, time_masking)\n",
        "\n",
        "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
        "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
        "        t0 = int(t0)\n",
        "        spec[:, t0:t0 + num_frames_to_mask] = value\n",
        "    return spec\n",
        "\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self,\n",
        "                 num_mask=2,\n",
        "                 freq_masking=0.15,\n",
        "                 time_masking=0.20):\n",
        "        self.num_mask = num_mask\n",
        "        self.freq_masking = freq_masking\n",
        "        self.time_masking = time_masking\n",
        "\n",
        "    def __call__(self, image,**kwargs):\n",
        "        return spec_augment(image,\n",
        "                            self.num_mask,\n",
        "                            self.freq_masking,\n",
        "                            self.time_masking,\n",
        "                            image.min())\n",
        "        \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh6HCuLTht4F"
      },
      "source": [
        "### Get transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3IpGNumrqfN"
      },
      "source": [
        "def get_transforms(train, height,width,\n",
        "                   wrap_pad_prob=0.5,\n",
        "                   resize_scale=(1, 0.8),\n",
        "                   resize_ratio=(1, 2.4),\n",
        "                   resize_prob=0.4,\n",
        "                   spec_num_mask=2,\n",
        "                   spec_freq_masking=0.15,\n",
        "                   spec_time_masking=0.20,\n",
        "                   spec_prob=0.5):\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "    if train:\n",
        "      \n",
        "        transforms = albumentations.Compose([\n",
        "            \n",
        "            \n",
        "            \n",
        "            albumentations.OneOf([albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"constant\"),p=0.5),\n",
        "                                  albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=0.5),\n",
        "                                  #albumentations.Resize(height,width,p=0.6),\n",
        "                                  #albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=0.3),\n",
        "                                  #albumentations.RandomResizedCrop(height,width,scale=(1,0.9), ratio=(1,2.0),p=0.2),     \n",
        "                                    ],p=1),\n",
        "            \n",
        "            albumentations.Lambda(AudioCrop(percentage=0.9), p=AudioConfig.audio_crop_prob),\n",
        "            #albumentations.RandomCrop(height,width,p=1),\n",
        "            \n",
        "            #albumentations.OneOf([albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio),\n",
        "            #                      albumentations.RandomResizedCrop(height,width,scale=(1,0.9), ratio=(1,2.0)), ],p=resize_prob),\n",
        "            #albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=resize_prob),\n",
        "\n",
        "            \n",
        "            \n",
        "            #albumentations.Compose([albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=1),\n",
        "            #                        albumentations.RandomCrop(AudioConfig.n_mels,width,p=1),],p=1),\n",
        "            albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=0.0),\n",
        "            #albumentations.CenterCrop(AudioConfig.n_mels,width,p=1),\n",
        "            #albumentations.RandomCrop(AudioConfig.n_mels,width,p=1),\n",
        "            \n",
        "            #albumentations.Crop(x_min=0, y_min=0, x_max=width, y_max=AudioConfig.n_mels,p=1),\n",
        "            albumentations.Resize(height,width,p=1),\n",
        "            albumentations.OneOf([albumentations.Lambda(SpecAugment(num_mask=2,freq_masking=0.10,time_masking=0.16)),\n",
        "                                  #albumentations.Lambda(SpecAugment()) ,\n",
        "                                  ],p=AudioConfig.spec_aug_prob),\n",
        "                                  \n",
        "            \n",
        "            albumentations.Lambda(ImageStack(),p=1),\n",
        "            albumentations.Lambda(ToColor(),p=1),\n",
        "            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
        "            albumentations.pytorch.transforms.ToTensor(),\n",
        "            \n",
        "            \n",
        "        ])\n",
        "    else:\n",
        "        transforms = albumentations.Compose([\n",
        "            albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=1),\n",
        "            #albumentations.Crop(x_min=0, y_min=0, x_max=width, y_max=AudioConfig.n_mels,p=1),\n",
        "            albumentations.CenterCrop(AudioConfig.n_mels,width,p=1),\n",
        "            albumentations.Resize(height,width,p=1),\n",
        "            albumentations.Lambda(ImageStack(),p=1),\n",
        "            albumentations.Lambda(ToColor(),p=1),\n",
        "            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
        "            albumentations.pytorch.transforms.ToTensor(),\n",
        "            \n",
        "        ])\n",
        "    return transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jziAsIAj5umN"
      },
      "source": [
        "## Mixers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwNhBxJq5v0x"
      },
      "source": [
        "def get_random_sample(dataset):\n",
        "    rnd_idx = random.randint(0, len(dataset) - 1)\n",
        "    rnd_audio,rnd_target=dataset.tensor_dict[df.loc[rnd_idx,dataset.path_col]]\n",
        "    rnd_target=dataset.class_to_idx[rnd_target]\n",
        "    \n",
        "    rnd_audio = dataset.transform(image=rnd_audio[0])[\"image\"]\n",
        "    rnd_target=dataset.target_transform(rnd_target,num_classes=len(dataset.classes))\n",
        "    return rnd_audio, rnd_target\n",
        "\n",
        "class AddMixer:\n",
        "    def __init__(self, alpha_dist='uniform'):\n",
        "        assert alpha_dist in ['uniform', 'beta']\n",
        "        self.alpha_dist = alpha_dist\n",
        "\n",
        "    def sample_alpha(self):\n",
        "        if self.alpha_dist == 'uniform':\n",
        "            return random.uniform(0, 0.5)\n",
        "        elif self.alpha_dist == 'beta':\n",
        "            return np.random.beta(0.4, 0.4)\n",
        "\n",
        "    def __call__(self, dataset, image, target):\n",
        "        rnd_image, rnd_target = get_random_sample(dataset)\n",
        "        alpha = self.sample_alpha()\n",
        "        image = (1 - alpha) * image + alpha * rnd_image\n",
        "        target = (1 - alpha) * target + alpha * rnd_target\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class SigmoidConcatMixer:\n",
        "    def __init__(self, sigmoid_range=(3, 12)):\n",
        "        self.sigmoid_range = sigmoid_range\n",
        "\n",
        "    def sample_mask(self, size):\n",
        "        x_radius = random.randint(*self.sigmoid_range)\n",
        "\n",
        "        step = (x_radius * 2) / size[1]\n",
        "        x = np.arange(-x_radius, x_radius, step=step)\n",
        "        y = torch.sigmoid(torch.from_numpy(x)).numpy()\n",
        "        mix_mask = np.tile(y, (size[0], 1))\n",
        "        return torch.from_numpy(mix_mask.astype(np.float32))\n",
        "\n",
        "    def __call__(self, dataset, image, target):\n",
        "        rnd_image, rnd_target = get_random_sample(dataset)\n",
        "\n",
        "        mix_mask = self.sample_mask(image.shape[-2:])\n",
        "        rnd_mix_mask = 1 - mix_mask\n",
        "\n",
        "        image = mix_mask * image + rnd_mix_mask * rnd_image\n",
        "        target = target + rnd_target\n",
        "        target = np.clip(target, 0.0, 1.0)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomMixer:\n",
        "    def __init__(self, mixers, p=None):\n",
        "        self.mixers = mixers\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, dataset, image, target):\n",
        "        mixer = np.random.choice(self.mixers, p=self.p)\n",
        "        image, target = mixer(dataset, image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class UseMixerWithProb:\n",
        "    def __init__(self, mixer, prob=.5):\n",
        "        self.mixer = mixer\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, dataset, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            return self.mixer(dataset, image, target)\n",
        "            print(image.shape,target.shape)\n",
        "        return image, target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DinrkdkYBhNf"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSNZgkPJ9XWs"
      },
      "source": [
        "!unzip audio_files.zip >>/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7uWHmNP9YCY"
      },
      "source": [
        "!unzip AdditionalUtterances.zip >>/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFlfmqHu9ceu"
      },
      "source": [
        "!unzip nlp_keywords_29Oct2020.zip >>/dev/null\n",
        "!rm 'nlp_keywords/.DS_Store'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYmN4kE-JAyS"
      },
      "source": [
        "df1=pd.read_csv(\"Train.csv\")\n",
        "submission=pd.read_csv(\"SampleSubmission.csv\")\n",
        "submission[\"label\"]=\"akawuka\"\n",
        "df2,_=create_csv_dataset_from_path(Config.DATASET2_PATH)\n",
        "df3,_=create_csv_dataset_from_path(Config.DATASET3_PATH)\n",
        "df=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)\n",
        "#df=df1\n",
        "df_all=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36rXFfKSGR5_",
        "outputId": "d00cecde-2648-444b-9d83-21ffa19f0768"
      },
      "source": [
        "print(\"Files in the dataset : \",len(os.listdir(Config.DATASET_PATH)))\n",
        "print(\"train1 shape : \",df1.shape)\n",
        "print(\"train2 shape : \",df2.shape)\n",
        "print(\"train3 shape : \",df3.shape)\n",
        "print(\"train all shape : \",df.shape)\n",
        "print(\"test shape : \",submission.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files in the dataset :  2126\n",
            "train1 shape :  (1109, 2)\n",
            "train2 shape :  (1740, 2)\n",
            "train3 shape :  (1860, 2)\n",
            "train all shape :  (4709, 2)\n",
            "test shape :  (1017, 195)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lerCPU6BGZ4C"
      },
      "source": [
        "classes=df[\"label\"].unique()\n",
        "class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n",
        "idx_to_class={idx:classe for idx,classe in enumerate(classes)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNOiHN30vu_G"
      },
      "source": [
        "# Dataset definition and loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcBYAbEwLuVS"
      },
      "source": [
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, df, loader,classes=None, transform=None,\n",
        "                 target_transform=None,device=torch.device(\"cpu\")):\n",
        "        super(Dataset, self).__init__()\n",
        "        \n",
        "        self.df=df.reset_index(drop=True)\n",
        "        self.loader=loader\n",
        "        self.transform=transform\n",
        "        \n",
        "        self.target_transform=target_transform\n",
        "        self.device=device\n",
        "        self.loaded=False\n",
        "        self.loaded_samples=[]\n",
        "        self.path_col=\"fn\"\n",
        "        self.target_col=\"label\"\n",
        "        if classes is None:\n",
        "            self.classes=df[self.target_col].unique()\n",
        "        else :\n",
        "            self.classes=classes\n",
        "        \n",
        "        self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n",
        "        self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n",
        "        \n",
        "    \n",
        "    def load_data(self):\n",
        "        self.loaded_samples=[]\n",
        "        for ind in tqdm(range(len(self.df)),0):\n",
        "            path=self.df.loc[ind,self.path_col]\n",
        "            target=self.df.loc[ind,self.target_col]\n",
        "            sample = self.loader(path)\n",
        "            self.loaded_samples.append([sample,target])\n",
        "        self.loaded=True\n",
        "        \n",
        "    def save_tensor(self,path):\n",
        "        assert self.loaded==True\n",
        "        torch.save(self.loaded_samples,path)\n",
        "    def load_tensor(self,path):\n",
        "        self.loaded_samples=torch.load(path)\n",
        "        self.loaded=True\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            dict {\"audio\" \"sample_rate\" \"target\"}\n",
        "        \"\"\"\n",
        "        if self.loaded:\n",
        "            sample, target = self.loaded_samples[index]\n",
        "        \n",
        "        else:    \n",
        "            path=self.df.loc[index,self.path_col]\n",
        "            target=self.df.loc[index,self.target_col]\n",
        "            sample = self.loader(path)\n",
        "\n",
        "        audio=sample[0]\n",
        "        sample_rate=sample[1]\n",
        "        target=self.class_to_idx[target]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            audio = self.transform(image=audio)[\"image\"]\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "        return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH4xyFDV_yKa"
      },
      "source": [
        "class PathDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, tensor_dict,classes=None, transform=None,\n",
        "                  target_transform=None,mixer=None,device=torch.device(\"cpu\")):\n",
        "          super(PathDataset, self).__init__()\n",
        "          \n",
        "          self.df=df.reset_index(drop=True)\n",
        "          self.tensor_dict=tensor_dict\n",
        "          self.transform=transform\n",
        "          self.mixer=mixer\n",
        "          self.target_transform=target_transform\n",
        "          self.device=device\n",
        "          self.path_col=\"fn\"\n",
        "          self.target_col=\"label\"\n",
        "          if classes is None:\n",
        "              self.classes=df[self.target_col].unique()\n",
        "          else :\n",
        "              self.classes=classes\n",
        "          \n",
        "          self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n",
        "          self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              index (int): Index\n",
        "\n",
        "          Returns:\n",
        "              dict {\"audio\" \"sample_rate\" \"target\"}\n",
        "          \"\"\"\n",
        "          \n",
        "          path=self.df.loc[index,self.path_col]\n",
        "          #target=self.df.loc[index,self.target_col]\n",
        "          sample,target = self.tensor_dict[path]\n",
        "          audio=sample[0]\n",
        "          sample_rate=sample[1]\n",
        "          target=class_to_idx[target]\n",
        "          \n",
        "          if self.transform is not None:\n",
        "              audio = self.transform(image=audio)[\"image\"]\n",
        "\n",
        "          if self.target_transform is not None:\n",
        "              target = self.target_transform(target,num_classes=len(self.classes))\n",
        "\n",
        "          \n",
        "          if self.mixer is not None:\n",
        "              audio, target = self.mixer(self, audio, target)\n",
        "\n",
        "          \n",
        "          return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoL2RSWjB1ho"
      },
      "source": [
        "train_transform = get_transforms(train=True,height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClD_PHK_63YZ"
      },
      "source": [
        "amixer = RandomMixer([\n",
        "        SigmoidConcatMixer(sigmoid_range=(3, 12)),\n",
        "        AddMixer(alpha_dist='uniform')\n",
        "    ], p=[0.6, 0.4])\n",
        "amixer = UseMixerWithProb(amixer, prob=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4l0T4PnPrKa"
      },
      "source": [
        "audio_set=CSVDataset(df_all,read_as_melspectrogram,classes=classes,transform=train_transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrha5HiNmwYI"
      },
      "source": [
        "submission_set=CSVDataset(submission,read_as_melspectrogram,classes=classes,transform=train_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2-o95eNKVUZ"
      },
      "source": [
        "audio_set.load_data()\n",
        "#audio_set.load_tensor(\"dataset.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnzdXCN9B7NL"
      },
      "source": [
        "tensor_dict={path:audio_set.loaded_samples[idx] for idx,path in enumerate(df_all[audio_set.path_col])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzXOWW_0Cz-i"
      },
      "source": [
        "loaded_set=PathDataset(df_all,tensor_dict,classes=classes,transform=train_transform,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jNUNeYcD7R-",
        "outputId": "3e7d73c5-d275-43bd-b422-df16531d2982"
      },
      "source": [
        "loaded_set.__getitem__(457)[\"audio\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 400, 228])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUnnVgyub9jb"
      },
      "source": [
        "submission_set.load_data()\n",
        "#submission_set.load_tensor(\"submission.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw_moWkWcDdn"
      },
      "source": [
        "submission_tensor_dict={path:submission_set.loaded_samples[idx] for idx,path in enumerate(submission[submission_set.path_col])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apxsiVm2pad6"
      },
      "source": [
        "# Class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdpk0JI9sqbJ"
      },
      "source": [
        "df1[\"num_label\"]=df1.label.apply(lambda x:class_to_idx[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP4rQaujpZ0K",
        "outputId": "782bd6fa-30e5-4bd6-d449-37671609f7ba"
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 classes,\n",
        "                                                 df1.label)\n",
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['akawuka' 'banana' 'obulwadde' 'nnyaanya' 'pampu' 'obutunda' 'plantation'\n",
            " 'ensujju' 'okulimibwa' 'mpeke' 'okusaasaana' 'ebigimusa' 'ekikolo' 'farm'\n",
            " 'kisaanyi' 'kikajjo' 'ekisaanyi' 'ndwadde' 'omusiri' 'butterfly'\n",
            " 'munyeera' 'eggobe' 'ebiwojjolo' 'ebisoolisooli' 'namuginga' 'okugimusa'\n",
            " 'maize streak virus' 'ekirime' 'miceere' 'sikungula' 'lumonde'\n",
            " 'okukungula' 'cassava' 'ebirime' 'ebijanjaalo' 'weeding' 'garden'\n",
            " 'drought' 'leaves' 'insect' 'akatungulu' 'seed' 'pepper'\n",
            " 'matooke seedlings' 'harvesting' 'medicine' 'nursery bed' 'mucungwa'\n",
            " 'endwadde' 'pawpaw' 'enkota' 'ensiringanyi' 'kassooli' 'okufuuyira'\n",
            " 'caterpillars' 'ekijanjaalo' 'okukkoola' 'crop' 'okulima' 'endagala'\n",
            " 'kaamulali' 'ennima' 'omuceere' 'micungwa' 'ebisaanyi' 'plant' 'eddagala'\n",
            " 'ennimiro' 'amakoola' 'ebiwuka' 'ekigimusa' 'bibala' 'beans' 'nnimiro'\n",
            " 'ebinyebwa' 'passion fruit' 'Spinach' 'okuzifuuyira' 'ekirwadde'\n",
            " 'nakavundira' 'nfukirira' 'onion' 'ddagala' 'muwogo' 'irrigate'\n",
            " 'akasaanyi' 'ekikajjo' 'emmwanyi' 'ekiwojjolo' 'orange' 'ebibala'\n",
            " 'ebyobulimi' 'ensuku' 'farmer' 'spray' 'obumonde' 'nnasale beedi'\n",
            " 'abalimi' 'okusaasaanya' 'doodo' 'enva endiirwa' 'ebikolo' 'obusaanyi'\n",
            " 'omulimisa' 'muceere' 'ejjobyo' 'ebikajjo' 'omucungwa' 'amappapaali'\n",
            " 'ensigo' 'ebikoola' 'emboga' 'spread' 'akamonde' 'kasaanyi' 'dig'\n",
            " 'ebisooli' 'nnakati' 'obulimi' 'mangoes' 'sweet potatoes' 'akammwanyi'\n",
            " 'vegetables' 'worm' 'amakungula' 'omuyembe' 'harvest' 'olusuku'\n",
            " 'amalagala' 'npk' 'kikolo' 'maize' 'coffee' 'ebijjanjalo'\n",
            " 'irish potatoes' 'ebimera' 'matooke' 'leaf' 'afukirira' 'ensukusa'\n",
            " 'caterpillar' 'sukumawiki' 'suckers' 'amatooke' 'emiyembe' 'endokwa'\n",
            " 'okusimba' 'mulimi' 'farming instructor' 'fertilizer' 'kukungula'\n",
            " 'akatunda' 'omulimi' 'nambaale' 'ebikongoliro' 'sow' 'ground nuts'\n",
            " 'super grow' 'ekimera' 'fruit picking' 'obuwuka' 'okusiga' 'emisiri'\n",
            " 'ekitooke' 'emicungwa' 'pumpkin' 'greens' 'bulimi' 'agriculture'\n",
            " 'okufukirira' 'tomatoes' 'fruit' 'ebitooke' 'rice' 'ebbugga' 'ppaapaali'\n",
            " 'okunnoga' 'obutungulu' 'ennyaanya' 'lusuku' 'insects' 'mango'\n",
            " 'eppapaali' 'Pump' 'maize stalk borer' 'ekibala' 'watermelon' 'ekyeya'\n",
            " 'disease' 'ekikoola' 'faamu' 'cabbages' 'sugarcane'], y=0           akawuka\n",
            "1            banana\n",
            "2         obulwadde\n",
            "3          nnyaanya\n",
            "4             pampu\n",
            "           ...     \n",
            "1104        cassava\n",
            "1105     harvesting\n",
            "1106           farm\n",
            "1107    nakavundira\n",
            "1108    nursery bed\n",
            "Name: label, Length: 1109, dtype: object as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5746114 , 1.4365285 , 0.63845711, 0.95768566, 1.1492228 ,\n",
              "       0.82087343, 0.71826425, 0.95768566, 1.1492228 , 0.95768566,\n",
              "       0.63845711, 0.47884283, 1.4365285 , 0.82087343, 0.71826425,\n",
              "       1.4365285 , 0.63845711, 0.522374  , 1.1492228 , 1.91537133,\n",
              "       0.522374  , 1.1492228 , 0.522374  , 0.82087343, 0.5746114 ,\n",
              "       0.5746114 , 1.4365285 , 1.1492228 , 0.95768566, 0.95768566,\n",
              "       0.95768566, 0.95768566, 1.91537133, 1.1492228 , 0.95768566,\n",
              "       0.71826425, 0.82087343, 0.95768566, 0.82087343, 1.1492228 ,\n",
              "       0.95768566, 0.82087343, 1.4365285 , 1.91537133, 0.82087343,\n",
              "       1.4365285 , 0.82087343, 1.1492228 , 0.522374  , 1.4365285 ,\n",
              "       0.95768566, 0.522374  , 0.95768566, 0.5746114 , 1.4365285 ,\n",
              "       0.95768566, 1.4365285 , 0.71826425, 1.1492228 , 0.95768566,\n",
              "       0.82087343, 1.1492228 , 1.4365285 , 1.1492228 , 0.63845711,\n",
              "       0.82087343, 0.63845711, 1.1492228 , 1.4365285 , 0.522374  ,\n",
              "       0.71826425, 0.95768566, 1.91537133, 1.1492228 , 1.1492228 ,\n",
              "       1.91537133, 1.4365285 , 0.5746114 , 0.71826425, 0.63845711,\n",
              "       1.1492228 , 1.91537133, 0.63845711, 0.95768566, 0.82087343,\n",
              "       0.522374  , 1.1492228 , 0.95768566, 0.71826425, 1.91537133,\n",
              "       1.1492228 , 1.1492228 , 1.4365285 , 0.82087343, 1.4365285 ,\n",
              "       0.95768566, 1.1492228 , 1.4365285 , 0.63845711, 0.95768566,\n",
              "       0.82087343, 1.4365285 , 0.63845711, 1.1492228 , 0.95768566,\n",
              "       0.71826425, 0.95768566, 1.1492228 , 0.95768566, 1.1492228 ,\n",
              "       1.1492228 , 1.1492228 , 1.4365285 , 1.1492228 , 0.5746114 ,\n",
              "       0.82087343, 0.82087343, 0.95768566, 1.4365285 , 1.91537133,\n",
              "       1.91537133, 0.82087343, 1.91537133, 1.91537133, 1.1492228 ,\n",
              "       0.95768566, 0.82087343, 1.1492228 , 0.95768566, 1.4365285 ,\n",
              "       1.4365285 , 1.4365285 , 1.4365285 , 1.1492228 , 1.91537133,\n",
              "       0.95768566, 0.95768566, 0.71826425, 1.1492228 , 0.95768566,\n",
              "       1.1492228 , 1.91537133, 0.95768566, 1.1492228 , 0.95768566,\n",
              "       1.91537133, 1.1492228 , 1.4365285 , 0.82087343, 1.1492228 ,\n",
              "       1.4365285 , 0.95768566, 1.1492228 , 0.82087343, 1.4365285 ,\n",
              "       0.82087343, 1.91537133, 1.4365285 , 1.4365285 , 0.82087343,\n",
              "       0.5746114 , 1.4365285 , 1.4365285 , 1.4365285 , 0.71826425,\n",
              "       1.91537133, 1.4365285 , 1.1492228 , 0.82087343, 1.4365285 ,\n",
              "       1.91537133, 0.82087343, 0.95768566, 1.4365285 , 1.4365285 ,\n",
              "       1.1492228 , 1.4365285 , 0.95768566, 0.95768566, 1.1492228 ,\n",
              "       1.4365285 , 1.91537133, 0.95768566, 0.95768566, 1.91537133,\n",
              "       1.91537133, 1.4365285 , 1.91537133, 1.91537133, 1.4365285 ,\n",
              "       1.91537133, 1.91537133, 1.91537133])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swdduFm2_BZh"
      },
      "source": [
        "# Create folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqgyHUwk_DFS",
        "outputId": "ca6934cb-cf15-43ec-d510-b26f48dd680d"
      },
      "source": [
        "df[\"folds\"]=-1\n",
        "df2[\"folds\"]=-1\n",
        "df3[\"folds\"]=-1\n",
        "\n",
        "kf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.random_state, shuffle=False)\n",
        "for fold, (_, val_index) in enumerate(kf.split(df,df[\"label\"])):\n",
        "        df.loc[val_index, \"folds\"] = fold"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfuD3RXRig61"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppGrcXJEiiat"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self,arch,num_classes ,pretrained='imagenet'):\n",
        "        super(Net, self).__init__()\n",
        "        self.base_model = pretrainedmodels.__dict__[\n",
        "            arch\n",
        "        ](pretrained=pretrained)\n",
        "        \n",
        "        \n",
        "        self.prepare = torch.nn.Sequential()\n",
        "        self.prepare.add_module('conv', nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1, stride=1,\n",
        "                                                bias=False))\n",
        "        #self.prepare.add_module('bn', nn.BatchNorm2d(3, eps=0.001, momentum=0.1, affine=True))\n",
        "        if arch==\"dpn98\":\n",
        "            self.l0 = torch.nn.Linear(2688, num_classes)\n",
        "        elif arch==\"se_resnext50_32x4d\" or arch==\"resnet101\" :\n",
        "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
        "        elif arch==\"dpn68\":\n",
        "            self.l0 = torch.nn.Linear(832, num_classes)\n",
        "        elif arch==\"resnet18\":\n",
        "            self.l0 = torch.nn.Linear(512, num_classes)\n",
        "            #self.l0 = torch.nn.Linear(1024, num_classes)\n",
        "\n",
        "        elif arch==\"vgg19\":\n",
        "            self.l0 = torch.nn.Linear(512, num_classes)\n",
        "            #self.l0 = torch.nn.Linear(1024, num_classes)\n",
        "\n",
        "        elif arch==\"se_resnet50\":    \n",
        "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
        "        elif arch==\"resnet50\":    \n",
        "            self.l0 = torch.nn.Linear(2048, num_classes)  \n",
        "        elif arch==\"senet154\":\n",
        "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
        "        elif arch==\"se_resnext101_32x4d\":\n",
        "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
        "        elif arch==\"dpn107\":\n",
        "            self.l0 = torch.nn.Linear(2688, num_classes)\n",
        "        elif arch==\"densenet121\":\n",
        "            self.l0 = torch.nn.Linear(1024, num_classes)\n",
        "            fc_size = self.base_model.last_linear.in_features\n",
        "            #print(\"fc_size : \",fc_size)\n",
        "            self.base_model.last_linear = nn.Sequential(nn.Linear(7168, 193))\n",
        "\n",
        "\n",
        "        else :\n",
        "            self.l0 = torch.nn.Linear(4098, num_classes)\n",
        "    def forward(self, audio, target,sample_rate):\n",
        "        batch_size, _, _, _ = audio.shape\n",
        "        \n",
        "        x=audio\n",
        "        x = self.base_model.features(x)\n",
        "        \n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
        "        \n",
        "        out = self.l0(x)\n",
        "        \n",
        "        loss = torch.nn.CrossEntropyLoss()(out, torch.argmax(target, dim=1))\n",
        "        \n",
        "\n",
        "        return out, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hu_LCWchgVr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pnh8tcz5gVp"
      },
      "source": [
        "model_name=\"dpn68\"\n",
        "#model_name=\"dpn98\"\n",
        "#model_name=\"resnet18\"\n",
        "#model_name=\"densenet121\"\n",
        "pretrained=\"imagenet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkfdfenPC1kS"
      },
      "source": [
        "model = Net(model_name,num_classes=len(classes),pretrained=pretrained)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5XRMa6M1WKr"
      },
      "source": [
        "def train(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
        "    \n",
        "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
        "    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n",
        "    \n",
        "    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    valid_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    mixer = RandomMixer([\n",
        "        \n",
        "        AddMixer(alpha_dist='uniform')\n",
        "    ], p=[1])\n",
        "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
        "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
        "    )\n",
        "    \n",
        "    \n",
        "    \n",
        "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    \n",
        "    model.to(Config.device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, factor=0.6,min_lr=Config.min_lr,verbose=True)\n",
        "    \n",
        "    es = EarlyStopping(patience=8, mode=\"min\")\n",
        "    eng = Engine(model, optimizer, device=Config.device)\n",
        "    for epoch in range(Config.epochs):\n",
        "        train_loss = eng.train(train_loader)\n",
        "        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
        "        \n",
        "        with open('out.txt', 'a') as f:\n",
        "            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n",
        "        \n",
        "        scheduler.step(valid_loss)\n",
        "        es(valid_loss, model, model_path=model_path)\n",
        "        if es.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tminX_NX_hia"
      },
      "source": [
        "def after_train(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    model_save_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
        "    \n",
        "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
        "    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n",
        "    \n",
        "    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    valid_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    mixer = RandomMixer([\n",
        "        \n",
        "        AddMixer(alpha_dist='uniform')\n",
        "    ], p=[1])\n",
        "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
        "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
        "    )\n",
        "    \n",
        "    \n",
        "    \n",
        "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(Config.device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.aftertrain_lr)\n",
        "    \n",
        "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0.2*1e-6, last_epoch=-1,)\n",
        "    \n",
        "    es = EarlyStopping(patience=6, mode=\"min\")\n",
        "    eng = Engine(model, optimizer,scheduler=scheduler, device=Config.device)\n",
        "    for epoch in range(Config.epochs):\n",
        "        if epoch!=0:\n",
        "            train_loss = eng.train(train_loader)\n",
        "        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
        "        \n",
        "        with open('out.txt', 'a') as f:\n",
        "            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n",
        "        \n",
        "        \n",
        "        es(valid_loss, model, model_path=model_save_path)\n",
        "        if es.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgvg-XQQEf2_"
      },
      "source": [
        "def eval_train(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
        "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
        "    \n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    valid_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    mixer = RandomMixer([\n",
        "        \n",
        "        AddMixer(alpha_dist='uniform')\n",
        "    ], p=[1])\n",
        "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
        "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(Config.device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
        "    eng = Engine(model, optimizer, device=Config.device)\n",
        "    \n",
        "    train_loss,predictions = eng.evaluate(train_loader, return_predictions=True)\n",
        "    valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
        "    \n",
        "    \n",
        "    print(f\"train loss = {train_loss}, valid loss = {valid_loss} \")\n",
        "    return train_loss,valid_loss\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fZjwth7AYZ"
      },
      "source": [
        "def predict(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    test_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    \n",
        "    \n",
        "    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=test_transfrom,target_transform=to_categorical)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(Config.device)\n",
        "    model.eval()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr)\n",
        "    eng = Engine(model, optimizer, device=Config.device)\n",
        "    predictions = eng.predict(test_loader)\n",
        "    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQPt_MTu5OA"
      },
      "source": [
        "def predict_tta(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    test_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    \n",
        "    \n",
        "    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(Config.device)\n",
        "    model.eval()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
        "    eng = Engine(model, optimizer, device=Config.device)\n",
        "    all_predictions=[]\n",
        "    for i in range(30):\n",
        "        all_predictions.append(torch.nn.Softmax(dim=1)(torch.cat(eng.predict(test_loader))).numpy())\n",
        "    \n",
        "    predictions=gmean(all_predictions)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EudD6O6ILrfi"
      },
      "source": [
        "def generate_submission_csv(fold):\n",
        "    seed_all(num_seed)\n",
        "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
        "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
        "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
        "    \n",
        "    train_transfrom = get_transforms(train=True,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    valid_transfrom = get_transforms(train=False,\n",
        "                                     height=AudioConfig.height,\n",
        "                                     width=AudioConfig.width,\n",
        "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
        "    mixer = RandomMixer([\n",
        "        \n",
        "        AddMixer(alpha_dist='uniform')\n",
        "    ], p=[1])\n",
        "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
        "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
        "    \n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
        "    )\n",
        "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(Config.device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
        "    eng = Engine(model, optimizer, device=Config.device)\n",
        "    \n",
        "    \n",
        "    predictions = eng.predict(valid_loader)\n",
        "    \n",
        "    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n",
        "    sample = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
        "    \n",
        "    sample.loc[:, classes] = predictions\n",
        "    return sample\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgmL6qprRPS9"
      },
      "source": [
        "11:54"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOGbYimYlsb"
      },
      "source": [
        "### Training folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEDczcmboOhY"
      },
      "source": [
        "%%capture\n",
        "for fold in range(0,Config.n_folds):\n",
        "    print(\"Fold : \",fold)\n",
        "    train(fold)\n",
        "    after_train(fold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ce57903de3874bf18c9899217b74db0a",
            "462a2960475140d0b9721c3b95de72b0",
            "236cdec74e30498e815d651419c8ea72",
            "c0d7bb06b09e4847a8ee891228084a27",
            "f7931ebdfef74e2e8406a90809f857f0",
            "b43623c1684e4108ad97a4f3805116a9",
            "578f24703bde402eae518729dfe6ccdf",
            "b87842c960e743d3a069dc79a371d343",
            "227e0fe850cc43a29f22434e929c8ee1",
            "87ea195ff9ed4a05acf219d73c2bc0e5",
            "d172778d1d224470b86c91d0c890331f",
            "c436b55cc9494597b0ab97b127786114",
            "4f189f39f1574dd7ac3e680b12cb9eff",
            "26e70b0cbce140c4a26b7c2927f7c44f",
            "83bd79d8af2140e6aba06cc113f4eb3d",
            "d2f107cff31340c09f14b992823523c0",
            "9ea61c24e635408ca094005cb05686c1",
            "8182960514b148dca017c93ad6b0ad39",
            "d4bf1e898f8f48d38f088a92c37de22a",
            "a64257bc746044daa4b6db8566209195"
          ]
        },
        "id": "l_58wRow9N_f",
        "outputId": "e7ce8053-c63e-4807-a3cd-04ef95200d04"
      },
      "source": [
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "for fold in range(Config.n_folds):\n",
        "    train_loss,valid_loss=eval_train(fold)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce57903de3874bf18c9899217b74db0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "462a2960475140d0b9721c3b95de72b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.12384458328696091, valid loss = 1.187116670897783 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "236cdec74e30498e815d651419c8ea72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d7bb06b09e4847a8ee891228084a27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.12307793921582348, valid loss = 0.7459480790243784 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7931ebdfef74e2e8406a90809f857f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43623c1684e4108ad97a4f3805116a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.08145786040942492, valid loss = 0.5414975580874776 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "578f24703bde402eae518729dfe6ccdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b87842c960e743d3a069dc79a371d343",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.05222611909441596, valid loss = 0.49727889978064427 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "227e0fe850cc43a29f22434e929c8ee1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87ea195ff9ed4a05acf219d73c2bc0e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.06339400710921321, valid loss = 0.42139920602420394 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d172778d1d224470b86c91d0c890331f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c436b55cc9494597b0ab97b127786114",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.05416902270058103, valid loss = 0.43164998805316873 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f189f39f1574dd7ac3e680b12cb9eff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26e70b0cbce140c4a26b7c2927f7c44f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.07797903835741618, valid loss = 0.23521054835775 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83bd79d8af2140e6aba06cc113f4eb3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f107cff31340c09f14b992823523c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.06480202990594235, valid loss = 0.2807040279104682 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ea61c24e635408ca094005cb05686c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8182960514b148dca017c93ad6b0ad39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.06779549509014512, valid loss = 0.14814615108554205 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4bf1e898f8f48d38f088a92c37de22a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a64257bc746044daa4b6db8566209195",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train loss = 0.05680204783891655, valid loss = 0.1526668972801417 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt6dIbaOK4qx",
        "outputId": "b715c521-c7c6-41e9-aca8-70a72fc1b327"
      },
      "source": [
        "np.mean(valid_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4641618026501558"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7aK8tRv1bj0",
        "colab": {
          "referenced_widgets": [
            "7ff27e5bcfe5497b8fe9c427a49961cd",
            "efbc80ac33404a2dbe6475fe5bd80050",
            "28437e6a960f4e368271d7740306ccfb",
            "04c3b8dec0ae4538b13fe6dd982df5f1",
            "357a28d10c4d4fa1a59a668cc96fdb0a",
            "10387626372b42438fd96aa318b21d2c",
            "60ae8f68a4bb4b21b7cf1d07da93bea7",
            "98b29a5553134bab8f6c9be9f66ad0db",
            "30589284319145949d446374ad0a392c",
            "05d5bbc663d6480eae612424f3b48d0e"
          ]
        },
        "outputId": "760b5cda-c265-4dc3-ac12-da06e2cca814"
      },
      "source": [
        "p=[]\n",
        "for fold in range(Config.n_folds):\n",
        "    p.append(predict(fold).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ff27e5bcfe5497b8fe9c427a49961cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efbc80ac33404a2dbe6475fe5bd80050",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28437e6a960f4e368271d7740306ccfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04c3b8dec0ae4538b13fe6dd982df5f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "357a28d10c4d4fa1a59a668cc96fdb0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10387626372b42438fd96aa318b21d2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60ae8f68a4bb4b21b7cf1d07da93bea7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98b29a5553134bab8f6c9be9f66ad0db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30589284319145949d446374ad0a392c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05d5bbc663d6480eae612424f3b48d0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "693biZU0ptws"
      },
      "source": [
        "predictions=gmean(p,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sxD_ihkLgCCg",
        "outputId": "026d98c4-b5ce-4e9e-baf2-3db89f851500"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1017, 193)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ-fHW9EIC0M"
      },
      "source": [
        "prediction_file=f\"dpn68.csv\"\n",
        "sample = pd.read_csv(\"SampleSubmission.csv\")\n",
        "sample.loc[:, classes] = predictions\n",
        "sample.to_csv(prediction_file, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL-IRKiTgCe2",
        "outputId": "d9610a44-004c-430d-c7a2-6f5c9a5b4e31"
      },
      "source": [
        "sample.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fn</th>\n",
              "      <th>maize streak virus</th>\n",
              "      <th>disease</th>\n",
              "      <th>okukkoola</th>\n",
              "      <th>muwogo</th>\n",
              "      <th>mpeke</th>\n",
              "      <th>mucungwa</th>\n",
              "      <th>greens</th>\n",
              "      <th>garden</th>\n",
              "      <th>mango</th>\n",
              "      <th>...</th>\n",
              "      <th>kasaanyi</th>\n",
              "      <th>suckers</th>\n",
              "      <th>insects</th>\n",
              "      <th>fertilizer</th>\n",
              "      <th>nakavundira</th>\n",
              "      <th>ekiwojjolo</th>\n",
              "      <th>akawuka</th>\n",
              "      <th>ddagala</th>\n",
              "      <th>ebiwojjolo</th>\n",
              "      <th>obutungulu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>audio_files/00118N3.wav</td>\n",
              "      <td>4.982374e-04</td>\n",
              "      <td>2.936035e-05</td>\n",
              "      <td>1.333825e-05</td>\n",
              "      <td>3.919042e-06</td>\n",
              "      <td>2.633253e-05</td>\n",
              "      <td>8.151552e-06</td>\n",
              "      <td>1.120957e-03</td>\n",
              "      <td>9.313720e-03</td>\n",
              "      <td>2.835080e-03</td>\n",
              "      <td>...</td>\n",
              "      <td>2.567300e-06</td>\n",
              "      <td>1.674182e-04</td>\n",
              "      <td>7.750516e-05</td>\n",
              "      <td>5.945370e-04</td>\n",
              "      <td>8.618630e-07</td>\n",
              "      <td>9.448920e-07</td>\n",
              "      <td>7.436629e-05</td>\n",
              "      <td>4.465838e-04</td>\n",
              "      <td>2.752912e-07</td>\n",
              "      <td>1.438401e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>audio_files/00P0NMV.wav</td>\n",
              "      <td>4.956156e-10</td>\n",
              "      <td>3.945247e-10</td>\n",
              "      <td>1.653673e-10</td>\n",
              "      <td>4.860184e-10</td>\n",
              "      <td>2.686990e-09</td>\n",
              "      <td>8.620225e-09</td>\n",
              "      <td>2.600239e-10</td>\n",
              "      <td>1.438830e-08</td>\n",
              "      <td>9.735198e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>2.857995e-09</td>\n",
              "      <td>3.664304e-09</td>\n",
              "      <td>3.635627e-10</td>\n",
              "      <td>7.494829e-08</td>\n",
              "      <td>9.991038e-01</td>\n",
              "      <td>8.150544e-09</td>\n",
              "      <td>3.170091e-07</td>\n",
              "      <td>8.116902e-07</td>\n",
              "      <td>1.520849e-09</td>\n",
              "      <td>8.244088e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>audio_files/01QEEZI.wav</td>\n",
              "      <td>3.727014e-07</td>\n",
              "      <td>2.840558e-06</td>\n",
              "      <td>1.689248e-06</td>\n",
              "      <td>9.056070e-05</td>\n",
              "      <td>1.190358e-06</td>\n",
              "      <td>7.335290e-07</td>\n",
              "      <td>4.384382e-07</td>\n",
              "      <td>7.363206e-07</td>\n",
              "      <td>1.020058e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>8.333769e-07</td>\n",
              "      <td>1.431371e-08</td>\n",
              "      <td>2.882434e-07</td>\n",
              "      <td>2.873111e-07</td>\n",
              "      <td>3.088863e-07</td>\n",
              "      <td>7.111642e-07</td>\n",
              "      <td>4.648003e-08</td>\n",
              "      <td>3.707795e-07</td>\n",
              "      <td>5.742815e-06</td>\n",
              "      <td>9.233232e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>audio_files/037YAED.wav</td>\n",
              "      <td>2.327935e-05</td>\n",
              "      <td>9.565458e-06</td>\n",
              "      <td>7.227104e-08</td>\n",
              "      <td>1.340523e-06</td>\n",
              "      <td>3.220598e-04</td>\n",
              "      <td>3.877383e-06</td>\n",
              "      <td>3.179390e-06</td>\n",
              "      <td>4.161313e-01</td>\n",
              "      <td>1.187128e-05</td>\n",
              "      <td>...</td>\n",
              "      <td>9.186658e-06</td>\n",
              "      <td>3.707174e-03</td>\n",
              "      <td>8.858315e-05</td>\n",
              "      <td>4.840702e-04</td>\n",
              "      <td>8.907200e-06</td>\n",
              "      <td>1.463914e-06</td>\n",
              "      <td>9.296805e-05</td>\n",
              "      <td>3.652636e-04</td>\n",
              "      <td>1.415184e-06</td>\n",
              "      <td>3.810170e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>audio_files/0382N0Y.wav</td>\n",
              "      <td>7.332573e-08</td>\n",
              "      <td>1.563314e-06</td>\n",
              "      <td>7.388575e-08</td>\n",
              "      <td>4.681138e-07</td>\n",
              "      <td>2.524634e-07</td>\n",
              "      <td>5.709543e-07</td>\n",
              "      <td>7.408599e-08</td>\n",
              "      <td>7.139526e-08</td>\n",
              "      <td>6.322460e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>1.688224e-07</td>\n",
              "      <td>4.939552e-08</td>\n",
              "      <td>5.181317e-08</td>\n",
              "      <td>1.411171e-07</td>\n",
              "      <td>8.881476e-07</td>\n",
              "      <td>1.860517e-06</td>\n",
              "      <td>2.594567e-06</td>\n",
              "      <td>7.256855e-08</td>\n",
              "      <td>1.065122e-05</td>\n",
              "      <td>1.903768e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 194 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        fn  maize streak virus       disease     okukkoola  \\\n",
              "0  audio_files/00118N3.wav        4.982374e-04  2.936035e-05  1.333825e-05   \n",
              "1  audio_files/00P0NMV.wav        4.956156e-10  3.945247e-10  1.653673e-10   \n",
              "2  audio_files/01QEEZI.wav        3.727014e-07  2.840558e-06  1.689248e-06   \n",
              "3  audio_files/037YAED.wav        2.327935e-05  9.565458e-06  7.227104e-08   \n",
              "4  audio_files/0382N0Y.wav        7.332573e-08  1.563314e-06  7.388575e-08   \n",
              "\n",
              "         muwogo         mpeke      mucungwa        greens        garden  \\\n",
              "0  3.919042e-06  2.633253e-05  8.151552e-06  1.120957e-03  9.313720e-03   \n",
              "1  4.860184e-10  2.686990e-09  8.620225e-09  2.600239e-10  1.438830e-08   \n",
              "2  9.056070e-05  1.190358e-06  7.335290e-07  4.384382e-07  7.363206e-07   \n",
              "3  1.340523e-06  3.220598e-04  3.877383e-06  3.179390e-06  4.161313e-01   \n",
              "4  4.681138e-07  2.524634e-07  5.709543e-07  7.408599e-08  7.139526e-08   \n",
              "\n",
              "          mango  ...      kasaanyi       suckers       insects    fertilizer  \\\n",
              "0  2.835080e-03  ...  2.567300e-06  1.674182e-04  7.750516e-05  5.945370e-04   \n",
              "1  9.735198e-10  ...  2.857995e-09  3.664304e-09  3.635627e-10  7.494829e-08   \n",
              "2  1.020058e-07  ...  8.333769e-07  1.431371e-08  2.882434e-07  2.873111e-07   \n",
              "3  1.187128e-05  ...  9.186658e-06  3.707174e-03  8.858315e-05  4.840702e-04   \n",
              "4  6.322460e-08  ...  1.688224e-07  4.939552e-08  5.181317e-08  1.411171e-07   \n",
              "\n",
              "    nakavundira    ekiwojjolo       akawuka       ddagala    ebiwojjolo  \\\n",
              "0  8.618630e-07  9.448920e-07  7.436629e-05  4.465838e-04  2.752912e-07   \n",
              "1  9.991038e-01  8.150544e-09  3.170091e-07  8.116902e-07  1.520849e-09   \n",
              "2  3.088863e-07  7.111642e-07  4.648003e-08  3.707795e-07  5.742815e-06   \n",
              "3  8.907200e-06  1.463914e-06  9.296805e-05  3.652636e-04  1.415184e-06   \n",
              "4  8.881476e-07  1.860517e-06  2.594567e-06  7.256855e-08  1.065122e-05   \n",
              "\n",
              "     obutungulu  \n",
              "0  1.438401e-06  \n",
              "1  8.244088e-09  \n",
              "2  9.233232e-07  \n",
              "3  3.810170e-06  \n",
              "4  1.903768e-07  \n",
              "\n",
              "[5 rows x 194 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_WsXTmiy-Z",
        "outputId": "45f36a26-58ab-480a-f7fb-09d1f9087eac"
      },
      "source": [
        "sample.iloc[700,1:].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9982855086786987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P041nF6hwo_m",
        "outputId": "532dac91-e764-4748-e6aa-81dc9089888d"
      },
      "source": [
        "sample.iloc[700,1:].max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9969573616981506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE1O2f9yL5cW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}