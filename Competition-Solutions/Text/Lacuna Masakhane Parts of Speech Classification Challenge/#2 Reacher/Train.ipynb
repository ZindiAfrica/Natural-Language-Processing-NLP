{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed75a58a-cd24-404f-a39e-875a1fe47d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  4 20:00:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 41%   40C    P8    24W / 140W |      1MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1a0d5-7f99-443d-8127-8a5fce4370fa",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ddcaa1-cf25-4fed-b993-c5c064218cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.21.3 torch==1.12.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43649c2d-64f1-4fc8-8b4c-8f21ff4472c4",
   "metadata": {},
   "source": [
    "## Clone Repos containing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c518cf-388b-4116-9ad7-41d4b607e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'masakhane-pos' already exists and is not an empty directory.\n",
      "fatal: destination path 'lacuna_pos_ner' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/masakhane-io/masakhane-pos.git\n",
    "!git clone https://github.com/masakhane-io/lacuna_pos_ner.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fe45d-d582-4182-b7a2-c8210674fe7f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c72c82-c95b-4b34-939b-d55b8d2062e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.model import *\n",
    "from src.dataset import *\n",
    "from src.train import *\n",
    "from src.infer import *\n",
    "from src.postprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e02c5-fd2b-46e6-95a5-77877bd971f7",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022d6b6b-5a76-42c2-b0bd-a641c0ca6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c529f10-8979-4c58-8cac-acf2bfc7d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PS: place Test.csv from Zindi in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b4510d-9211-49c1-b9a7-069dc0706861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train & test\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "test = preprocess_test(test)\n",
    "train = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae7946e5-c830-4eca-b2cf-8e9f09806591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hukumar wasan motsa jiki ta duniya WA ta cire...</td>\n",
       "      <td>[NOUN, NOUN, VERB, NOUN, SCONJ, NOUN, PROPN, A...</td>\n",
       "      <td>hau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hukumar ta duniya ta dauki wannan matakin ne ...</td>\n",
       "      <td>[NOUN, SCONJ, NOUN, AUX, VERB, PRON, NOUN, PAR...</td>\n",
       "      <td>hau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   Hukumar wasan motsa jiki ta duniya WA ta cire...   \n",
       "1   Hukumar ta duniya ta dauki wannan matakin ne ...   \n",
       "\n",
       "                                              target lang  \n",
       "0  [NOUN, NOUN, VERB, NOUN, SCONJ, NOUN, PROPN, A...  hau  \n",
       "1  [NOUN, SCONJ, NOUN, AUX, VERB, PRON, NOUN, PAR...  hau  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0f4d5c-f3a9-44b0-ad5b-ab922c5a2640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ne otim penj e kind Februar tarik 9 gi Februar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sifuna ne ojiwo jonyuol kod joma moko marito n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Ne otim penj e kind Februar tarik 9 gi Februar...\n",
       "1  Sifuna ne ojiwo jonyuol kod joma moko marito n..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf8c394-1b79-4ac2-9b34-99d8d6e39295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare monolingual data for pseudotagging\n",
    "luo = create_luo_data()\n",
    "tsn = create_tsn_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9fffb3-3340-4b83-a3a7-b0609ee4f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le fa ba lebagane le dikgwetlho di le dintsi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ramolotja ke mongwe wa ba ba fetang dimilione ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Le fa ba lebagane le dikgwetlho di le dintsi, ...\n",
       "1  Ramolotja ke mongwe wa ba ba fetang dimilione ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsn.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d8d6a-b20b-4dc5-b358-569b11181202",
   "metadata": {},
   "source": [
    "## Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e624b21-d99c-4a15-9f12-6130a7fd1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_languages = [\"zul\", \"xho\", \"sna\", \"lug\", \"kin\", \"swa\", \"nya\", \"sna\",\"bbj\",\"wol\", \"ibo\", \"pcm\"]#, \"eng-ron-wol\"]\n",
    "train = train[train.lang.isin(selected_languages)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7831d06a-3be9-44be-92a2-850abacd1e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.lang.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0cb1506-3dcf-45e8-acb4-272cd12cf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_mapper = {\n",
    "    \"ibo\": 0, \"nya\": 0,\n",
    "    \"kin\": 1, \"wol\": 1,\n",
    "    \"lug\": 2, \"swa\": 2, \"zul\": 2,\n",
    "    \"pcm\": 3, \"bbj\": 3,\n",
    "    \"xho\": 4, \"sna\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05c496b-365a-4bbf-b171-cf8ec22a7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"fold\"] = train.lang.map(folds_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5929cbce-9531-486a-925f-27de222e5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['ADP', 'AUX', 'VERB', 'PART', 'PRON', 'NOUN', 'PROPN', 'ADJ', 'PUNCT', 'X', 'NUM', 'ADV', 'DET', 'CCONJ', 'INTJ', 'SCONJ', 'SYM']\n",
    "labels_to_ids = {v:k for k,v in enumerate(labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f866a588-cb8d-4f35-8bb2-ef68421227dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lang</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ndị uweojii na steeti Edo ejidela ma kpọchie ...</td>\n",
       "      <td>[NOUN, NOUN, ADP, NOUN, PROPN, VERB, CCONJ, VE...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ihe ọdachi a bụ nke dapụtàrà n' obodo Ama dị ...</td>\n",
       "      <td>[NOUN, NOUN, DET, VERB, NOUN, VERB, ADP, NOUN,...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dịka ozi a kụpụrụ site n' aka onye na- ahụ ma...</td>\n",
       "      <td>[ADP, NOUN, PRON, VERB, ADP, ADP, NOUN, NOUN, ...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ọ gara n' ihu kọwaa na ndị nta ahụ sòrò ndị b...</td>\n",
       "      <td>[PRON, VERB, ADP, NOUN, VERB, SCONJ, NOUN, NOU...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O kwuru na oge e ruzịrị n' obodo ahụ , na ndị...</td>\n",
       "      <td>[PRON, VERB, SCONJ, NOUN, PRON, VERB, ADP, NOU...</td>\n",
       "      <td>ibo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16478</th>\n",
       "      <td>Dembare iri kutarisirawo zvekare kuti Ngezi P...</td>\n",
       "      <td>[PROPN, AUX, VERB, ADJ, DET, PROPN, PROPN, PRO...</td>\n",
       "      <td>sna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16479</th>\n",
       "      <td>Asi chikwata cha Tonderayi Ndiraya ichi chino...</td>\n",
       "      <td>[SCONJ, NOUN, ADP, PROPN, PROPN, DET, VERB, AD...</td>\n",
       "      <td>sna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16480</th>\n",
       "      <td>Asi mutambo mukuru svondo rino uchange uri ku...</td>\n",
       "      <td>[SCONJ, NOUN, ADJ, NOUN, DET, AUX, AUX, ADP, P...</td>\n",
       "      <td>sna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16481</th>\n",
       "      <td>Pa mwedzi miviri yapfuura Bosso yakunda mu mu...</td>\n",
       "      <td>[ADP, NOUN, NUM, ADJ, PROPN, VERB, ADP, NOUN, ...</td>\n",
       "      <td>sna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16482</th>\n",
       "      <td>Mimwe mitambo iriko yakamira seizvi : How Min...</td>\n",
       "      <td>[ADJ, NOUN, AUX, VERB, ADV, PUNCT, PROPN, PROP...</td>\n",
       "      <td>sna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16483 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0       Ndị uweojii na steeti Edo ejidela ma kpọchie ...   \n",
       "1       Ihe ọdachi a bụ nke dapụtàrà n' obodo Ama dị ...   \n",
       "2       Dịka ozi a kụpụrụ site n' aka onye na- ahụ ma...   \n",
       "3       Ọ gara n' ihu kọwaa na ndị nta ahụ sòrò ndị b...   \n",
       "4       O kwuru na oge e ruzịrị n' obodo ahụ , na ndị...   \n",
       "...                                                  ...   \n",
       "16478   Dembare iri kutarisirawo zvekare kuti Ngezi P...   \n",
       "16479   Asi chikwata cha Tonderayi Ndiraya ichi chino...   \n",
       "16480   Asi mutambo mukuru svondo rino uchange uri ku...   \n",
       "16481   Pa mwedzi miviri yapfuura Bosso yakunda mu mu...   \n",
       "16482   Mimwe mitambo iriko yakamira seizvi : How Min...   \n",
       "\n",
       "                                                  target lang  fold  \n",
       "0      [NOUN, NOUN, ADP, NOUN, PROPN, VERB, CCONJ, VE...  ibo     0  \n",
       "1      [NOUN, NOUN, DET, VERB, NOUN, VERB, ADP, NOUN,...  ibo     0  \n",
       "2      [ADP, NOUN, PRON, VERB, ADP, ADP, NOUN, NOUN, ...  ibo     0  \n",
       "3      [PRON, VERB, ADP, NOUN, VERB, SCONJ, NOUN, NOU...  ibo     0  \n",
       "4      [PRON, VERB, SCONJ, NOUN, PRON, VERB, ADP, NOU...  ibo     0  \n",
       "...                                                  ...  ...   ...  \n",
       "16478  [PROPN, AUX, VERB, ADJ, DET, PROPN, PROPN, PRO...  sna     4  \n",
       "16479  [SCONJ, NOUN, ADP, PROPN, PROPN, DET, VERB, AD...  sna     4  \n",
       "16480  [SCONJ, NOUN, ADJ, NOUN, DET, AUX, AUX, ADP, P...  sna     4  \n",
       "16481  [ADP, NOUN, NUM, ADJ, PROPN, VERB, ADP, NOUN, ...  sna     4  \n",
       "16482  [ADJ, NOUN, AUX, VERB, ADV, PUNCT, PROPN, PROP...  sna     4  \n",
       "\n",
       "[16483 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e1f29bc-2dc8-4e3a-8def-03af8a1838f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0         [ibo, nya]\n",
       "1         [kin, wol]\n",
       "2    [lug, swa, zul]\n",
       "3         [pcm, bbj]\n",
       "4         [xho, sna]\n",
       "Name: lang, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('fold').lang.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d588b-354c-4bca-8f4e-a2a35335c41b",
   "metadata": {},
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c004129-e2c6-4af6-b1d7-9db445d858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "NUM_EPOCHS = 4\n",
    "NUM_CORES = 4\n",
    "BATCH_SIZE = 8\n",
    "USE_FP16 = True\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "MAX_SEQ_LENGTH = 200\n",
    "PRETRAINED_MODEL = \"Davlan/afro-xlmr-large-61L\"\n",
    "USE_AMP = True\n",
    "LRs = [1e-5, 7e-6, 3e-6, 1e-6, 2.5e-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ea851-77ad-4f56-8092-39d732f80e19",
   "metadata": {},
   "source": [
    "## Round 1: Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d07b9ea-e971-428e-8031-cefb5d92bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Fold 0\n",
      "-------------------------------------------------------------\n",
      "FULL Dataset: (16483, 4)\n",
      "TRAIN Dataset: (13423, 4)\n",
      "TEST Dataset: (3060, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1678/1678 [05:45<00:00,  4.85it/s, train_loss=1.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 1.2027424631087515\n",
      "### Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 765/765 [00:29<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7845581312751827\n",
      "### Training epoch: 2\n",
      "### LR = 7e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1678/1678 [05:46<00:00,  4.84it/s, train_loss=0.922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.9215961636107357\n",
      "### Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 765/765 [00:29<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7896637467388844\n",
      "### Training epoch: 3\n",
      "### LR = 3e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1678/1678 [05:46<00:00,  4.85it/s, train_loss=0.878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.877718549590287\n",
      "### Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 765/765 [00:29<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7906613268628924\n",
      "### Training epoch: 4\n",
      "### LR = 1e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1678/1678 [05:46<00:00,  4.85it/s, train_loss=0.861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8608500977025697\n",
      "### Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 765/765 [00:29<00:00, 26.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7914010734420178\n",
      "-------------------------------------------------------------\n",
      "Fold 1\n",
      "-------------------------------------------------------------\n",
      "FULL Dataset: (16483, 4)\n",
      "TRAIN Dataset: (13408, 4)\n",
      "TEST Dataset: (3075, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1676/1676 [05:45<00:00,  4.85it/s, train_loss=1.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 1.2150700338771634\n",
      "### Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 769/769 [00:29<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7759401056210022\n",
      "### Training epoch: 2\n",
      "### LR = 7e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1676/1676 [05:46<00:00,  4.84it/s, train_loss=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.9575566037031233\n",
      "### Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 769/769 [00:29<00:00, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7624295094299401\n",
      "### Training epoch: 3\n",
      "### LR = 3e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1676/1676 [05:46<00:00,  4.84it/s, train_loss=0.913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.9127573213031013\n",
      "### Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 769/769 [00:29<00:00, 26.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7673558249804945\n",
      "### Training epoch: 4\n",
      "### LR = 1e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1676/1676 [05:46<00:00,  4.84it/s, train_loss=0.898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.897671022427964\n",
      "### Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 769/769 [00:29<00:00, 26.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7663374355736212\n",
      "-------------------------------------------------------------\n",
      "Fold 2\n",
      "-------------------------------------------------------------\n",
      "FULL Dataset: (16483, 4)\n",
      "TRAIN Dataset: (12130, 4)\n",
      "TEST Dataset: (4353, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1517/1517 [05:12<00:00,  4.85it/s, train_loss=1.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 1.202941944273821\n",
      "### Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1089/1089 [00:41<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7593399394422695\n",
      "### Training epoch: 2\n",
      "### LR = 7e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1517/1517 [05:13<00:00,  4.84it/s, train_loss=0.938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.9379383739312385\n",
      "### Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1089/1089 [00:41<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7562860690945726\n",
      "### Training epoch: 3\n",
      "### LR = 3e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1517/1517 [05:13<00:00,  4.84it/s, train_loss=0.893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8928392900002969\n",
      "### Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1089/1089 [00:41<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7516767326610561\n",
      "### Training epoch: 4\n",
      "### LR = 1e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1517/1517 [05:13<00:00,  4.84it/s, train_loss=0.879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8790543639274828\n",
      "### Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1089/1089 [00:41<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7542504858105699\n",
      "-------------------------------------------------------------\n",
      "Fold 3\n",
      "-------------------------------------------------------------\n",
      "FULL Dataset: (16483, 4)\n",
      "TRAIN Dataset: (13483, 4)\n",
      "TEST Dataset: (3000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=1.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 1.1340323978966524\n",
      "### Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 750/750 [00:28<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.6186567749933481\n",
      "### Training epoch: 2\n",
      "### LR = 7e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.894]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8935935080829182\n",
      "### Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 750/750 [00:28<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.62037869740936\n",
      "### Training epoch: 3\n",
      "### LR = 3e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8564739056541402\n",
      "### Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 750/750 [00:28<00:00, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.6225008331012589\n",
      "### Training epoch: 4\n",
      "### LR = 1e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8453518718940649\n",
      "### Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 750/750 [00:29<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.6259063653563263\n",
      "-------------------------------------------------------------\n",
      "Fold 4\n",
      "-------------------------------------------------------------\n",
      "FULL Dataset: (16483, 4)\n",
      "TRAIN Dataset: (13488, 4)\n",
      "TEST Dataset: (2995, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training epoch: 1\n",
      "### LR = 1e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=1.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 1.171622664690866\n",
      "### Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 749/749 [00:28<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7866081877683705\n",
      "### Training epoch: 2\n",
      "### LR = 7e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.9150980274377486\n",
      "### Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 749/749 [00:29<00:00, 25.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7899412427406663\n",
      "### Training epoch: 3\n",
      "### LR = 3e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8723491904715768\n",
      "### Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 749/749 [00:28<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.7903840736718573\n",
      "### Training epoch: 4\n",
      "### LR = 1e-06\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1686/1686 [05:48<00:00,  4.84it/s, train_loss=0.856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch: 0.8563931371888515\n",
      "### Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 749/749 [00:28<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ACC:  0.790448740059496\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p models/exp5\n",
    "train_folds(\n",
    "    train, \"exp5\",labels_to_ids, PRETRAINED_MODEL, LRs=LRs, BATCH_SIZE=BATCH_SIZE, LR=LR, NUM_EPOCHS=NUM_EPOCHS, GRAD_ACCUM_STEPS=GRAD_ACCUM_STEPS, label_smooth=0.1,\n",
    "    grad_norm=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0563eb7-b5d6-4859-8b8f-b7fcd8731e9c",
   "metadata": {},
   "source": [
    "### Create pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4397a530-6d48-4198-936f-bbf85075acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def create_pseudos(lang, df, exp):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "    test_params = {'batch_size': 4,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 4,\n",
    "                    'pin_memory':True,\n",
    "                   'collate_fn':DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                    } \n",
    "    \n",
    "    testing_set = CustomDataset(df, tokenizer,labels_to_ids, 200, True)\n",
    "    test_texts_loader = DataLoader(testing_set, **test_params)\n",
    "    for FOLD in range(5):\n",
    "        preds = inference(test_texts_loader,FOLD, exp, ids_to_labels, PRETRAINED_MODEL, lang)\n",
    "    \n",
    "    test_params = {'batch_size': 1,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 4,\n",
    "                    'pin_memory':True,\n",
    "                   'collate_fn':DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                    } \n",
    "    \n",
    "    testing_set = CustomDataset(df, tokenizer,labels_to_ids, 200, True)\n",
    "    test_texts_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    fold0 = np.load(f\"{lang}/TEST_fold0.npy\")\n",
    "    fold1 = np.load(f\"{lang}/TEST_fold1.npy\")\n",
    "    fold2 = np.load(f\"{lang}/TEST_fold2.npy\")\n",
    "    fold3 = np.load(f\"{lang}/TEST_fold3.npy\")\n",
    "    fold4 = np.load(f\"{lang}/TEST_fold4.npy\")\n",
    "    ensemble = (fold0 + fold1 + fold2 + fold3 + fold4 )/5\n",
    "    final_preds = folds_inference(test_texts_loader, ensemble, ids_to_labels)\n",
    "    from itertools import chain\n",
    "    l = list(chain.from_iterable(final_preds))\n",
    "    df[\"target\"] = l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1144cc50-5c90-48fb-b7fa-40975499483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc71e36d2d0496e9ebdfbd88b2d9cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0051baef5fc4bccbe2951904eb8d078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "100%|██████████| 1812/1812 [02:41<00:00, 11.20it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1812/1812 [02:54<00:00, 10.39it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1812/1812 [03:04<00:00,  9.80it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1812/1812 [03:07<00:00,  9.68it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1812/1812 [03:10<00:00,  9.52it/s]\n",
      "100%|██████████| 7246/7246 [00:12<00:00, 603.63it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "100%|██████████| 1034/1034 [01:46<00:00,  9.71it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1034/1034 [01:45<00:00,  9.81it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1034/1034 [01:44<00:00,  9.85it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1034/1034 [01:45<00:00,  9.81it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1034/1034 [01:44<00:00,  9.87it/s]\n",
      "100%|██████████| 4135/4135 [00:06<00:00, 607.10it/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p luo tsn\n",
    "luo = create_pseudos('luo', luo, 'exp5')\n",
    "luo.to_csv('luo/luo.csv', index=False)\n",
    "tsn = create_pseudos('tsn', tsn, 'exp5')\n",
    "tsn.to_csv('tsn/tsn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4810ba-2ff9-4937-a7ac-d2083a144e4c",
   "metadata": {},
   "source": [
    "## Round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e7e3b18-1258-40cb-9b87-41b3154ad6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#luo = pd.read_csv('luo/luo.csv').dropna().reset_index(drop=True)\n",
    "#luo[\"target\"] = luo[\"target\"].apply(convert_to_list)\n",
    "#\n",
    "#tsn = pd.read_csv('tsn/tsn.csv').dropna().reset_index(drop=True)\n",
    "#tsn[\"target\"] = tsn[\"target\"].apply(convert_to_list)\n",
    "#train = pd.read_csv('data/train.csv')\n",
    "#train[\"target\"] = train[\"target\"].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e361235-b288-4e67-b7ca-e850402189c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/exp6\n",
    "luo['fold'] = -1\n",
    "tsn['fold'] = -1\n",
    "train_folds(\n",
    "    train, \"exp6\",labels_to_ids, PRETRAINED_MODEL, LRs=LRs, BATCH_SIZE=BATCH_SIZE, LR=LR, NUM_EPOCHS=NUM_EPOCHS, GRAD_ACCUM_STEPS=GRAD_ACCUM_STEPS,\n",
    "    label_smooth=0.09,grad_norm=0, luo=luo, tsn=tsn, luo_n=3000, tsn_n=3000\n",
    ")\n",
    "luo = create_pseudos('luo', luo, 'exp6')\n",
    "luo.to_csv('luo/luo.csv', index=False)\n",
    "tsn = create_pseudos('tsn', tsn, 'exp6')\n",
    "tsn.to_csv('tsn/tsn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f950a-f41b-4264-9179-1c14e6193b41",
   "metadata": {},
   "source": [
    "## Round2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b830daa3-931a-4789-8af7-33e7e051ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def infer_test(lang, df, exp):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "    test_params = {'batch_size': 4,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 4,\n",
    "                    'pin_memory':True,\n",
    "                   'collate_fn':DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                    } \n",
    "    \n",
    "    testing_set = CustomDataset(df, tokenizer,labels_to_ids, 200, True)\n",
    "    test_texts_loader = DataLoader(testing_set, **test_params)\n",
    "    for FOLD in range(5):\n",
    "        preds = inference(test_texts_loader,FOLD, exp, ids_to_labels, PRETRAINED_MODEL, lang)\n",
    "    \n",
    "    test_params = {'batch_size': 1,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 4,\n",
    "                    'pin_memory':True,\n",
    "                   'collate_fn':DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                    } \n",
    "    \n",
    "    testing_set = CustomDataset(df, tokenizer,labels_to_ids, 200, True)\n",
    "    test_texts_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    fold0 = np.load(f\"{lang}/TEST_fold0.npy\")\n",
    "    fold1 = np.load(f\"{lang}/TEST_fold1.npy\")\n",
    "    fold2 = np.load(f\"{lang}/TEST_fold2.npy\")\n",
    "    fold3 = np.load(f\"{lang}/TEST_fold3.npy\")\n",
    "    fold4 = np.load(f\"{lang}/TEST_fold4.npy\")\n",
    "    ensemble = (fold0 + fold1 + fold2 + fold3 + fold4 )/5\n",
    "    final_preds = folds_inference(test_texts_loader, ensemble, ids_to_labels)\n",
    "    l = list(chain.from_iterable(final_preds))\n",
    "    df[\"target\"] = l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d23b7-8642-4bce-990b-ce769ebaea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/exp7\n",
    "luo['fold'] = -1\n",
    "tsn['fold'] = -1\n",
    "train_folds(\n",
    "    train, \"exp7\",labels_to_ids, PRETRAINED_MODEL, LRs=LRs, BATCH_SIZE=BATCH_SIZE, LR=LR, NUM_EPOCHS=NUM_EPOCHS, GRAD_ACCUM_STEPS=GRAD_ACCUM_STEPS,\n",
    "    label_smooth=0.08,grad_norm=0, luo=luo, tsn=tsn, luo_n=3600, tsn_n=3100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f22bf-ab5a-4580-b9ef-0757e2975242",
   "metadata": {},
   "outputs": [],
   "source": [
    "luo = create_pseudos('luo', luo.dropna().reset_index(drop=True), 'exp7').dropna().reset_index(drop=True)\n",
    "luo.to_csv('luo/luo.csv', index=False)\n",
    "tsn = create_pseudos('tsn', tsn.dropna().reset_index(drop=True), 'exp7').dropna().reset_index(drop=True)\n",
    "tsn.to_csv('tsn/tsn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc870e-6a39-48f7-9858-779723853278",
   "metadata": {},
   "source": [
    "## Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad41f1-009c-4991-9e69-99bedfe5e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/exp8\n",
    "luo['fold'] = -1\n",
    "tsn['fold'] = -1\n",
    "train_folds(\n",
    "    train, \"exp8\",labels_to_ids, PRETRAINED_MODEL, LRs=LRs, BATCH_SIZE=BATCH_SIZE, LR=LR, NUM_EPOCHS=NUM_EPOCHS, GRAD_ACCUM_STEPS=GRAD_ACCUM_STEPS,\n",
    "    label_smooth=0.08,grad_norm=0, luo=luo, tsn=tsn, luo_n=4000, tsn_n=3500\n",
    ")\n",
    "luo = create_pseudos('luo', luo, 'exp8')\n",
    "luo.to_csv('luo/luo.csv', index=False)\n",
    "tsn = create_pseudos('tsn', tsn, 'exp8')\n",
    "tsn.to_csv('tsn/tsn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424f77a-0158-417b-9ffd-0e602b7e38b2",
   "metadata": {},
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dd9fb-0dde-45f5-9626-5c03982394ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/exp9\n",
    "luo['fold'] = -1\n",
    "tsn['fold'] = -1\n",
    "\n",
    "train_folds(\n",
    "    train, \"exp9\",labels_to_ids, PRETRAINED_MODEL, LRs=LRs, BATCH_SIZE=BATCH_SIZE, LR=LR, NUM_EPOCHS=NUM_EPOCHS, GRAD_ACCUM_STEPS=GRAD_ACCUM_STEPS,\n",
    "    label_smooth=0.07,grad_norm=0, luo=luo, tsn=tsn, luo_n=4200, tsn_n=3700\n",
    ")\n",
    "luo = create_pseudos('luo', luo, 'exp9')\n",
    "luo.to_csv('luo/luo.csv', index=False)\n",
    "tsn = create_pseudos('tsn', tsn, 'exp9')\n",
    "tsn.to_csv('tsn/tsn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe772e0-066f-4638-b25d-5e6e07728cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d3ee1-1f66-41e5-aaf0-c08fd0f6add5",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34bfec0-d95d-42fb-8a2b-d0d0b9409406",
   "metadata": {},
   "source": [
    "### Nelder-Mead: This will take a considerable amount of time so you can skip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd39f7f-dc4b-4f52-9d48-2fc4f3a8db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold0_w = find_best_thresh(0, train, PRETRAINED_MODEL, labels_to_ids, 'exp6')\n",
    "#fold1_w = find_best_thresh(1, train, PRETRAINED_MODEL, labels_to_ids, 'exp6')\n",
    "#fold2_w = find_best_thresh(2, train, PRETRAINED_MODEL, labels_to_ids, 'exp6')\n",
    "#fold3_w = find_best_thresh(3, train, PRETRAINED_MODEL, labels_to_ids, 'exp6')\n",
    "#fold4_w = find_best_thresh(4, train, PRETRAINED_MODEL, labels_to_ids, 'exp6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "490c77a5-33fb-4d64-b344-a50cb3609397",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold0_w = np.array([0.9767511 , 0.3981664 , 1.61331843, 0.2595545 , 1.81338756,\n",
    "       1.28729398, 0.76055386, 0.09408014, 1.06771219, 1.47217587,\n",
    "       1.25157168, 1.03431401, 0.33491653, 0.52220695, 1.25210021,\n",
    "       1.6623188 , 1.14310743])\n",
    "fold1_w = np.array([0.9767511 , 0.3981664 , 1.61331843, 0.2595545 , 1.81338756,\n",
    "       1.28729398, 0.76055386, 0.09408014, 1.06771219, 1.47217587,\n",
    "       1.25157168, 1.03431401, 0.33491653, 0.52220695, 1.25210021,\n",
    "       1.6623188 , 1.14310743])\n",
    "fold2_w = np.array([0.87911696, 1.52908478, 0.81095612, 0.9107473 , 0.00344712,\n",
    "       1.16542127, 0.89979852, 0.96534758, 1.0812359 , 1.22360948,\n",
    "       1.01659991, 1.27223844, 1.12947197, 0.81287226, 1.21721596,\n",
    "       1.00092072, 1.16421219])\n",
    "\n",
    "fold3_w = np.array([0.9767511 , 0.3981664 , 1.61331843, 0.2595545 , 1.81338756,\n",
    "       1.28729398, 0.76055386, 0.09408014, 1.06771219, 1.47217587,\n",
    "       1.25157168, 1.03431401, 0.33491653, 0.52220695, 1.25210021,\n",
    "       1.6623188 , 1.14310743])\n",
    "\n",
    "fold4_w = np.array([1.18264067, 1.49253454, 0.6136005 , 1.0873054 , 0.64117687,\n",
    "       1.07551867, 1.32218085, 0.21402064, 1.14864329, 1.01481558,\n",
    "       1.08367489, 0.24427615, 1.31034121, 1.07435968, 0.76660614,\n",
    "       1.56849931, 1.01845596])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b4d430-989f-402e-9320-1579e4a7be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "100%|██████████| 302/302 [00:25<00:00, 11.74it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 302/302 [00:27<00:00, 11.10it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 302/302 [00:27<00:00, 10.81it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 302/302 [00:28<00:00, 10.56it/s]\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large-61L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-61L and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 302/302 [00:28<00:00, 10.65it/s]\n",
      "100%|██████████| 1208/1208 [00:01<00:00, 627.95it/s]\n"
     ]
    }
   ],
   "source": [
    "sub = infer_test('models/exp9', test, 'exp9')\n",
    "l = list(chain.from_iterable(sub.target.values))\n",
    "len(l)\n",
    "TEST = pd.read_csv(\"data/Test.csv\")\n",
    "TEST[\"Pos\"] = l\n",
    "TEST[[\"Id\", \"Pos\"]].to_csv('lacuna_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c48e88-1036-40df-adb2-e99e50bec328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:02<00:00, 586.81it/s]\n"
     ]
    }
   ],
   "source": [
    "fold0 = np.load(f\"models/exp9/TEST_fold0.npy\")*fold0_w\n",
    "fold1 = np.load(f\"models/exp9/TEST_fold1.npy\")*fold1_w\n",
    "fold2 = np.load(f\"models/exp9/TEST_fold2.npy\")*fold2_w\n",
    "fold3 = np.load(f\"models/exp9/TEST_fold3.npy\")*fold3_w\n",
    "fold4 = np.load(f\"models/exp9/TEST_fold4.npy\")*fold4_w \n",
    "ensemble = (fold0 + fold1 + fold2 + fold3 +fold4)/5\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "test_params = {'batch_size': 1,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 4,\n",
    "                'pin_memory':True,\n",
    "               'collate_fn':DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                } \n",
    "\n",
    "testing_set = CustomDataset(test, tokenizer, labels_to_ids, 200, True)\n",
    "test_texts_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "p = folds_inference(test_texts_loader, ensemble, ids_to_labels)\n",
    "from itertools import chain\n",
    "l = list(chain.from_iterable(p))\n",
    "l = list(chain.from_iterable(l))\n",
    "TEST = pd.read_csv(\"data/Test.csv\")\n",
    "TEST[\"Pos\"] = l\n",
    "TEST[[\"Id\", \"Pos\"]].to_csv('lacuna_opt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3cb2b-c778-479e-9734-3cd91dc6b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = postprocess(pd.read_csv('data/Test.csv'), pd.read_csv('lacuna_opt.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b5f39f9-983f-42f5-a4c3-10cdcfda057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('lacuna_opt_post.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
