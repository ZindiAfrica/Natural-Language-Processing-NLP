# Competition Summary

## Description

Machine translation (MT) is a popular Natural Language Processing (NLP) task which involves the automatic translation of sentences from a source language to a target language.

Machine translation models are very sensitive to the domain they were trained on which limit their generalization to multiple domains of interest like legal or medical domains. 

The problem is more severe in low-resource languages like Yorùbá where the most available datasets used for training are in the religious domain like JW300.

The goal of this challenge is to build a machine translation model to translate sentences from Yorùbá language to English language in several domains like news articles, daily conversations, spoken dialog transcripts and books.



## Competition Rules

Participation in this competition could be as an individual or in a team of up to four people.

Prizes are transferred only to the individual players or to the team leader.

Code was not shared privately outside of a team. Any code shared, was made available to all competition participants through the platform. (i.e. on the discussion boards).



## Datasets and packages

Only the datasets provided for this competition was used.

The training data consist of 10,054 parallel Yorùbá-English sentences from different domains like news, Yorùbá proverbs, movie transcript, ted talks, radio broadcast transcript, localization translation, and books.

Pretrained models were allowed as long as they are openly available to everyone.



## Submissions and winning

The top 3 solution placed on the final leaderboard were required to submit their winning solution code to us for verification, and thereby agreed to assign all worldwide rights of copyright in and to such winning solution to Zindi.



## Reproducibility

The full documentation was retrieved. This includes:
- All data used

- Output data and where they are stored

- Explanation of features used

- The solution must include the original data provided by Zindi and validated external data (no processed data)

- All editing of data must be done in a notebook (i.e. not manually in Excel)



## Data standards:

- The most recent versions of packages were used.

- Submitted code run on the original train, test, and other datasets provided.



## Evaluation:

The error metric for this competition is Rouge Score, ROUGE-N (N-gram) scoring (Rouge1), reporting the F-measure.

The Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scoring algorithm calculates the similarity between a candidate document and a collection of reference documents.

Use the ROUGE score to evaluate the quality of document translation and summarization models 



## Prizes

1st Place: $1 000 USD

2nd Place: $600 USD

3rd Place: $400 USD



## Benefits

The translation models developed will assist human translators in their jobs, help English speakers to have better communication with native speakers of Yorùbá, and improve the automatic translation of Yorùbá web pages to English language.


[![CC BY 4.0][cc-by-shield]][cc-by]

This work is licensed under a
[Creative Commons Attribution 4.0 International License][cc-by].

[![CC BY 4.0][cc-by-image]][cc-by]

[cc-by]: http://creativecommons.org/licenses/by/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png
[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg


