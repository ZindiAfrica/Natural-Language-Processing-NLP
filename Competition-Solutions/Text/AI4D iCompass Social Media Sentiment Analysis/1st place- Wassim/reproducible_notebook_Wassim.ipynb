{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "reproducible-notebook.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcZsJ8MGSlBR"
      },
      "source": [
        "# Wassim Henia Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geb6RU6ySlBT"
      },
      "source": [
        "<font size=\"4\"> Hello, this is my solution AI4D iCompass Social Media Sentiment Analysis for Tunisian Arabizi. Both the inference and the training are performed in one notebook. Both take less than 8 hours to training, meaning that they respect the time limit (8 Hours for training and 2 Hours for inference). </font>\n",
        "\n",
        "-------\n",
        "\n",
        "<font size=\"3\"> <b> Solution Overview: </b> The Tunisian dialect, and Maghrebi dialects generally, are a mosaic of different languages, namely: Arabic (To a large degree), French, English and a bit of Italian loan words. Using transfer learning along side with ensembling would boost the model's performance. A good ensemble would allow the understanding of different parts of a sentence that are crucial in determining the underlying sentiment of it. </font>\n",
        "\n",
        "<font size=\"3\"> However this poses many challenges. First, transfer learning models are huge, with millions of parameters. The eight hours ceiling would greatly limit the size of the ensemble. Second, all open source Bert-like Arabic models only support Arabic script, and the data is in Arabizi. Lastly, the dataset is relatively small, and somewhat biased, and using it to train a model that generalises well is difficult.</font>\n",
        "\n",
        "<font size=\"3\"> To fix this, my solution focused on speeding up data loading, and optimizing the training time. Finding a good validation strategy to assess the model's performance despite the small dataset, and to reduce bias. Crafting an algorithm to encode Arabizi text into Arabic characters. </font>\n",
        "\n",
        "<font size=\"4\">  <b>Important Note: </b></font> <font size=\"3\"> This notebook doesn't reproduce exactly the same score as in the leaderboard, however I would be the first regardless. This is because I trained the simple transformers model and the Fastai on Tesla P4 GPU for the leaderboard submission, meanwhile here, it is trained on a P100. Change of GPUs makes models not reproducible despite fixing the seed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3s6QDZFSlBU"
      },
      "source": [
        "<font size=\"3\"> Models that I have used:  </font>\n",
        "    \n",
        "    - Distill Bert multilingual (Simple Transformers)\n",
        "    - Fast AI text classifier\n",
        "    - Multilingual Bert (Huggingface)\n",
        "    - Camembert (Huggingface) for French understanding\n",
        "    - Distill Bert Multilingual (Huggingface)\n",
        "    - Arabic Dialect Bert (Hugginface)\n",
        "    \n",
        "<font size=\"3\"> Note that English bert models are trained on English wikipedia, meaning it understands common words in other languages, and this proves to be helpful in in inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NVhV6kG9SlBV"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WM2kRRHnSlBW"
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "123wVsJmSlBW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL \n",
        "from torch import nn\n",
        "import time\n",
        "import random\n",
        "from matplotlib import gridspec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NHkKgLx0SlBX"
      },
      "source": [
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import warnings \n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, Sampler\n",
        "from transformers import BertTokenizer, AutoModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9AVElShHSlBX"
      },
      "source": [
        "#Sorted sentences from shortest to longest to speed up the inference time\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "test.columns = [\"ID\", \"text\"]\n",
        "\n",
        "test[\"lens\"] = test.text.apply(len)\n",
        "test = test.sort_values(by=\"lens\")\n",
        "\n",
        "test_idxes = test.ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU3aHjJZSlBY"
      },
      "source": [
        "# Simple Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XlaDtfh1SlBY"
      },
      "source": [
        "!pip install simpletransformers -q\n",
        "\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gE0PuTHgSlBY"
      },
      "source": [
        "#Utility function that loads train and test in the same order\n",
        "def get_data():\n",
        "    \n",
        "    train = pd.read_csv(\"Train.csv\")\n",
        "    test = pd.read_csv(\"Test.csv\")\n",
        "    \n",
        "    train.columns = [\"ID\", \"text\", \"label\"]\n",
        "    test.columns = [\"ID\", \"text\"]\n",
        "    \n",
        "    test = test.set_index(\"ID\", drop=True).loc[test_idxes].reset_index()\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUC_EFOJSlBZ"
      },
      "source": [
        "<font size=\"3\"> This function `removeDuplicates` is useful. It transformes sentence from \"Chibkkk yesssser ta7kiii!!!\" to \"Chibk yeser ta7ki\" which improves accuracy in some models. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MUogJe6tSlBZ"
      },
      "source": [
        "def removeDuplicates(S): \n",
        "          \n",
        "    n = len(S)  \n",
        "      \n",
        "    # We don't need to do anything for  \n",
        "    # empty or single character string.  \n",
        "    if (n < 2) : \n",
        "        return\n",
        "          \n",
        "    # j is used to store index is result  \n",
        "    # string (or index of current distinct  \n",
        "    # character)  \n",
        "    j = 0\n",
        "      \n",
        "    # Traversing string  \n",
        "    for i in range(n):  \n",
        "          \n",
        "        # If current character S[i]  \n",
        "        # is different from S[j]  \n",
        "        if (S[j] != S[i]): \n",
        "            j += 1\n",
        "            S[j] = S[i]  \n",
        "      \n",
        "    # Putting string termination  \n",
        "    # character.  \n",
        "    j += 1\n",
        "    S = S[:j] \n",
        "    return \"\".join(S) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUllCaUsSlBa"
      },
      "source": [
        "<font size=\"3\"> Each model has its own fixed seed </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SNSs_DPnSlBa"
      },
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rIc20qBvSlBa"
      },
      "source": [
        "#Add one to the labels because you can't set a negative number to be the target in CrossEntropy loss\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "train[\"text\"]=train['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OS1DhVwtSlBd"
      },
      "source": [
        "#Utility function to get the model\n",
        "def get_model(model_type, model_name, n_epochs = 2, train_batch_size = 100, eval_batch_size = 64, seq_len = 120, lr = 2e-5):\n",
        "  model = ClassificationModel(model_type, model_name,num_labels=3, args={'train_batch_size':train_batch_size,\n",
        "                                                                         \"eval_batch_size\": eval_batch_size,\n",
        "                                                                         'reprocess_input_data': True,\n",
        "                                                                         'overwrite_output_dir': True,\n",
        "                                                                         'fp16': False,\n",
        "                                                                         'do_lower_case': False,\n",
        "                                                                         'num_train_epochs': n_epochs,\n",
        "                                                                         'max_seq_length': seq_len,\n",
        "                                                                         'manual_seed': 2,\n",
        "                                                                         \"learning_rate\":lr,\n",
        "                                                                         \"save_eval_checkpoints\": False,\n",
        "                                                                         \"save_model_every_epoch\": False,})\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AcdzDiKVSlBe"
      },
      "source": [
        "tmp = pd.DataFrame()\n",
        "tmp['text'] = train['text']\n",
        "tmp['labels'] = train['label']\n",
        "\n",
        "\n",
        "tmp_trn, tmp_val = train_test_split(tmp, test_size=0.1, random_state=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RNhXKNlHSlBe"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wbP2oWyJSlBe"
      },
      "source": [
        "fast_model = get_model('distilbert', 'distilbert-base-multilingual-cased', n_epochs=2,lr=2e-4,seq_len=150,train_batch_size=160)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "60QumXE_SlBf"
      },
      "source": [
        "#Training the model\n",
        "fast_model.train_model(tmp_trn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o995Ram6SlBf"
      },
      "source": [
        "# Fast AI "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEXCffjcUzkU"
      },
      "source": [
        "FastAI for NLP uses RNN architecture other than the bert arthitecture so it boosts the score in the ensemble."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jzI6aa6dSlBf"
      },
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GYHnFCy0SlBg"
      },
      "source": [
        "set_seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5KwVTtoXSlBg"
      },
      "source": [
        "train, test = get_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UtOpTZ4MSlBg"
      },
      "source": [
        "data = (TextList.from_df(train.append(test), cols='text')\n",
        "                .split_by_rand_pct(0.1,seed=7)\n",
        "                .label_for_lm()  \n",
        "                .databunch(bs=48))\n",
        "data.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GaT9QH4pSlBh"
      },
      "source": [
        "learn = language_model_learner(data,AWD_LSTM, drop_mult=0.8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X1aWH7BmSlBh"
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fn3T_aUUSlBh"
      },
      "source": [
        "learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7jxFgTPdSlBh"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aDquoYdWSlBi"
      },
      "source": [
        "learn.save_encoder('fine_tuned_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d9fZ6ssdSlBi"
      },
      "source": [
        "test_datalist = TextList.from_df(test, cols='text', vocab=data.vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CZJ9r0JHSlBi"
      },
      "source": [
        "data_clas = (TextList.from_df(train, cols='text', vocab=data.vocab)\n",
        "             .split_by_rand_pct(0.1,seed=7)\n",
        "             .label_from_df(cols= 'label')\n",
        "             .add_test(test_datalist)\n",
        "             .databunch(bs=32))\n",
        "\n",
        "data_clas.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x8gYChcMSlBi"
      },
      "source": [
        "learn_classifier = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)\n",
        "\n",
        "# load the encoder saved  \n",
        "learn_classifier.load_encoder('fine_tuned_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ikxiBQT4SlBj"
      },
      "source": [
        "learn_classifier.freeze()\n",
        "learn_classifier.lr_find()\n",
        "learn_classifier.recorder.plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ilxq_ocWSlBj"
      },
      "source": [
        "learn_classifier.fit_one_cycle(3, 2e-2, moms=(0.8,0.7))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UFfQkRbsSlBj"
      },
      "source": [
        "learn_classifier.freeze_to(-2)\n",
        "learn_classifier.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lj0YKaVZSlBj"
      },
      "source": [
        "learn_classifier.freeze_to(-3)\n",
        "learn_classifier.lr_find()\n",
        "learn_classifier.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yTdZ4LDPSlBk"
      },
      "source": [
        "learn_classifier.freeze_to(-3)\n",
        "learn_classifier.fit_one_cycle(4, slice(2e-5/(2.6**4),2e-5), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3fcKvtEuSlBk"
      },
      "source": [
        "learn_classifier.unfreeze()\n",
        "learn_classifier.lr_find()\n",
        "learn_classifier.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fakx7OdSSlBk"
      },
      "source": [
        "learn_classifier.fit_one_cycle(2, slice(2e-4/(2.6**4),2e-4), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krVBcxcRSlBk"
      },
      "source": [
        "# Bert-Base-Uncased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "O_3xFFRmSlBk"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-oFuR_JSlBl"
      },
      "source": [
        "<font size=\"3\"> Same as removeDuplicates but using regular expressions </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J6RnykABSlBl"
      },
      "source": [
        "def text_preprocessing(text): \n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text = re.sub(r'([a-g-i-z][a-g-i-z])\\1+', r'\\1', text)\n",
        "        \n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uUlYwSCPSlBl"
      },
      "source": [
        "train, test = get_data()\n",
        "train.label+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dJFAugauSlBl"
      },
      "source": [
        "train[\"text\"]=train['text'].apply(text_preprocessing)\n",
        "test[\"text\"]=test['text'].apply(text_preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JX4-QNm5SlBm"
      },
      "source": [
        "#Load the tokenizer for bert-base-uncased, and set the padding token\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "m8NBzj8DSlBm"
      },
      "source": [
        "#Utility function that tokenizes sentences, and clips sentences longer than 256, and adds special tokens and masks.\n",
        "#The return value is a tuple containing: list of all sentences, list of attention masks\n",
        "def preprocessing_for_bert(data, max_len=256):\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    tmp = tokenizer.encode(\"ab\")[-1]\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        encoding = tokenizer.encode(sentence)\n",
        "\n",
        "        if len(encoding) > max_len:\n",
        "            encoding = encoding[:max_len-1] + [tmp]\n",
        "\n",
        "        in_ids = encoding\n",
        "        att_mask = [1]*len(encoding)\n",
        "        \n",
        "        input_ids.append(in_ids)\n",
        "        attention_masks.append(att_mask)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VsjoEJNeSlBm"
      },
      "source": [
        "#Custom Pytorch dataset that inherits from utils.toch.data\n",
        "#It gets the tokenized sentence, the mask, and the label (in case of training), and the sentence length\n",
        "class BertDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, masks, label=None):\n",
        "        \n",
        "        self.data = data\n",
        "        self.masks = masks\n",
        "        \n",
        "        if label != None:\n",
        "            self.labels = label\n",
        "        else:\n",
        "            self.labels = None\n",
        "        \n",
        "        self.lengths = [len(i) for i in data]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels !=  None:\n",
        "            return (self.data[idx], self.masks[idx], self.labels[idx], self.lengths[idx])\n",
        "        else:  #For validation\n",
        "            return (self.data[idx], self.masks[idx], None, self.lengths[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_4agBp8DSlBm"
      },
      "source": [
        "#This data collator pads sentences to longest sentence in the batch. It does speed up data loading.\n",
        "def data_collator(data):\n",
        "    \n",
        "    sentence, mask, label, length = zip(*data)\n",
        "    \n",
        "    tensor_dim = max(length)\n",
        "    \n",
        "    out_sentence = torch.full((len(sentence), tensor_dim), dtype=torch.long, fill_value=pad)\n",
        "    out_mask = torch.zeros(len(sentence), tensor_dim, dtype=torch.long)\n",
        "\n",
        "    for i in range(len(sentence)):\n",
        "        \n",
        "        out_sentence[i][:len(sentence[i])] = torch.Tensor(sentence[i])\n",
        "        out_mask[i][:len(mask[i])] = torch.Tensor(mask[i])\n",
        "    \n",
        "    if label[0] != None:\n",
        "        return (out_sentence, out_mask, torch.Tensor(label).long())\n",
        "    else:\n",
        "        return (out_sentence, out_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BJ26gwDJSlBn"
      },
      "source": [
        "#This custom sampler is key in speeding up the training\n",
        "#It samples sentences with similar length together\n",
        "#This greatly reduces the paddings in the sentence, and saves up important computation\n",
        "#The sampler return the indices in order that the dataloader would use for creating the batches\n",
        "class KSampler(Sampler):\n",
        "\n",
        "    def __init__(self, data_source, batch_size):\n",
        "        self.lens = [x[1] for x in data_source]  #Stores the lengths of the sentences\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        idx = list(range(len(self.lens)))  #Indexes of the sentences\n",
        "        arr = list(zip(self.lens, idx))  #Array of tuples containing lengths of the sentence and its index\n",
        "\n",
        "        random.shuffle(arr)   #Randomly shuffle them\n",
        "        n = self.batch_size*100\n",
        "\n",
        "        iterator = []\n",
        "\n",
        "        for i in range(0, len(self.lens), n):\n",
        "            dt = arr[i:i+n]  #Get batch_size*100 element\n",
        "            dt = sorted(dt, key=lambda x: x[0])  #Sort them from shortest, to longest\n",
        "\n",
        "            for j in range(0, len(dt), self.batch_size):\n",
        "                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))  #Get and store the indices of every batch\n",
        "                iterator.append(indices)\n",
        "\n",
        "        random.shuffle(iterator) #Randomly shuffle the batches\n",
        "        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r49HPL6FSlBn"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# The bert Classifier\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 200, 3\n",
        "#768,100,3\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        print(model_name)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(H, D_out),      \n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KyXcqKEaSlBn"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "    #tried 1e-5/2e-5/6e-5/4e-/3/7/\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),#5e-5\n",
        "                      lr=6e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qtueJgfESlBo"
      },
      "source": [
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    \n",
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vnm-ciZuSlBo"
      },
      "source": [
        "def train_fn(model, train_dataloader, val_dataloader=None, fold=None, epochs=4, evaluation=False, prefix=\"\"):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    max_acc = -99\n",
        "    print(\"Start training, fold %d ...\\n\" % (fold))\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "                \n",
        "                preds = torch.argmax(logits, dim=1).flatten()\n",
        "                accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "                \n",
        "            if step%200 == 0 and step != 0 and epoch_i != 0 and epoch_i != 1: #Calculate validation accuracy every 200 steps\n",
        "                \n",
        "                print(\"-\"*70)\n",
        "\n",
        "                if evaluation == True:\n",
        "\n",
        "                    val_loss, val_accuracy = evaluate_fn(model, val_dataloader)\n",
        "                    \n",
        "                    if val_accuracy > max_acc:\n",
        "                        max_acc = val_accuracy\n",
        "                        torch.save(model, prefix+\"best_\"+str(fold))\n",
        "                        print(\"new max\")\n",
        "                        \n",
        "\n",
        "                    print(val_accuracy)\n",
        "                    \n",
        "                    print(\"-\"*70)\n",
        "                print(\"\\n\")\n",
        "                \n",
        "                model.train()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate_fn(model, val_dataloader)\n",
        "            \n",
        "            if val_accuracy > max_acc:\n",
        "                max_acc = val_accuracy\n",
        "                torch.save(model, prefix+\"best_\"+str(fold))\n",
        "                print(\"new max\")\n",
        "                \n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate_fn(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            \n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uvxqsPs8SlBp"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "swwjhdPYSlBp"
      },
      "source": [
        "MAX_LEN = 256\n",
        "\n",
        "#Preprocess data\n",
        "X, X_masks = preprocessing_for_bert(train['text'].values, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t4UyBT2wSlBp"
      },
      "source": [
        "def get_indices(arr, idxs):  #Helper function to get multiple indexes from a list\n",
        "    \n",
        "    output = []\n",
        "    for idx in idxs:\n",
        "        output.append(arr[idx])\n",
        "        \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v6MY-q9SlBq"
      },
      "source": [
        "<font size=\"3\"> To train every huggingface model, I used cross validation, saved the model of every fold that has the heighest score. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vmgNOE02SlBq"
      },
      "source": [
        "n = 5\n",
        "\n",
        "kfolds = KFold(n, True, 2020) \n",
        "fold = 0\n",
        "\n",
        "for train_ids, val_ids in kfolds.split(X):\n",
        "    \n",
        "    train_inputs, train_masks = get_indices(X, train_ids) , get_indices(X_masks, train_ids)\n",
        "    train_labels = train.label.values[train_ids]\n",
        "    \n",
        "    val_inputs, val_masks = get_indices(X, val_ids) , get_indices(X_masks, val_ids)\n",
        "    val_labels = train.label.values[val_ids]\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    \n",
        "    val_inputs, val_labels, val_masks = list(zip(*sorted(zip(val_inputs, val_labels, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
        "    val_inputs, val_labels, val_masks = list(val_inputs), list(val_labels), list(val_masks)\n",
        "    \n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = BertDataset(train_inputs, train_masks, train_labels)  #Use the custom dataset\n",
        "    train_sampler = KSampler(train_data, batch_size)  #Use the custom sampler\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)  #Use the custom collator\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = BertDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "    \n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "    train_fn(bert_classifier, train_dataloader, val_dataloader, fold= fold, epochs=3, evaluation=True, prefix=\"bert_\")\n",
        "    \n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH3W8CM1SlBq"
      },
      "source": [
        "<font size=\"3\"> Using custom samplers, dataset, and data collator improved training time from 90 minutes to 8 minutes. I used the same strategy for the rest of the models, only with a minor difference in the pre-processisng."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCIc9qU-SlBq"
      },
      "source": [
        "# Multi-lingual Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f0-933pWSlBr"
      },
      "source": [
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "set_seed()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mOG1bSNhSlBr"
      },
      "source": [
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "\n",
        "\n",
        "model_name = 'bert-base-multilingual-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CPRZkUOzSlBr"
      },
      "source": [
        "train[\"text\"]=train['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J60Jcz92SlBs"
      },
      "source": [
        "X, X_masks = preprocessing_for_bert(train['text'].values, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "taBxqTHSSlBs"
      },
      "source": [
        "n = 5\n",
        "\n",
        "kfolds = KFold(n, True, 2020) \n",
        "fold = 0\n",
        "\n",
        "best_accs = []\n",
        "\n",
        "for train_ids, val_ids in kfolds.split(X):\n",
        "    \n",
        "    train_inputs, train_masks = get_indices(X, train_ids) , get_indices(X_masks, train_ids)\n",
        "    train_labels = train.label.values[train_ids]\n",
        "    \n",
        "    val_inputs, val_masks = get_indices(X, val_ids) , get_indices(X_masks, val_ids)\n",
        "    val_labels = train.label.values[val_ids]\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    \n",
        "    val_inputs, val_labels, val_masks = list(zip(*sorted(zip(val_inputs, val_labels, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
        "    val_inputs, val_labels, val_masks = list(val_inputs), list(val_labels), list(val_masks)\n",
        "    \n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = BertDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = KSampler(train_data, batch_size)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = BertDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "    \n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "    train_fn(bert_classifier, train_dataloader, val_dataloader, fold= fold, epochs=3, evaluation=True, prefix=\"multi-bert_\")\n",
        "    \n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaxpm1oWSlBt"
      },
      "source": [
        "# Camembert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a_m5HtPKSlBt"
      },
      "source": [
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F0fjK1mkSlBt"
      },
      "source": [
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "\n",
        "\n",
        "model_name = 'camembert-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gyC-bA1tSlBt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 200, 3\n",
        "#768,100,3\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        print(model_name)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(H, D_out),      \n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F0mMqMzASlBu"
      },
      "source": [
        "X, X_masks = preprocessing_for_bert(train['text'].values, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "89kILo6sSlBu"
      },
      "source": [
        "n = 5\n",
        "\n",
        "kfolds = KFold(n, True, 2020) \n",
        "fold = 0\n",
        "\n",
        "best_accs = []\n",
        "\n",
        "for train_ids, val_ids in kfolds.split(X):\n",
        "    \n",
        "    train_inputs, train_masks = get_indices(X, train_ids) , get_indices(X_masks, train_ids)\n",
        "    train_labels = train.label.values[train_ids]\n",
        "    \n",
        "    val_inputs, val_masks = get_indices(X, val_ids) , get_indices(X_masks, val_ids)\n",
        "    val_labels = train.label.values[val_ids]\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    \n",
        "    val_inputs, val_labels, val_masks = list(zip(*sorted(zip(val_inputs, val_labels, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
        "    val_inputs, val_labels, val_masks = list(val_inputs), list(val_labels), list(val_masks)\n",
        "    \n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = BertDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = KSampler(train_data, batch_size)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = BertDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "    \n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "    train_fn(bert_classifier, train_dataloader, val_dataloader, fold= fold, epochs=3, evaluation=True, prefix=\"fr_\")\n",
        "    \n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GS90azSlBu"
      },
      "source": [
        "# Distill Bert Multi-lingual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "194okak0SlBu"
      },
      "source": [
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uxuuGAHfSlBv"
      },
      "source": [
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "train[\"text\"]=train['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "\n",
        "\n",
        "\n",
        "model_name = 'distilbert-base-multilingual-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q81KUr5ASlBv"
      },
      "source": [
        "X, X_masks = preprocessing_for_bert(train['text'].values, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QfdrpvxkSlBv"
      },
      "source": [
        "n = 5\n",
        "\n",
        "kfolds = KFold(n, True, 2020) \n",
        "fold = 0\n",
        "\n",
        "best_accs = []\n",
        "\n",
        "for train_ids, val_ids in kfolds.split(X):\n",
        "    \n",
        "    train_inputs, train_masks = get_indices(X, train_ids) , get_indices(X_masks, train_ids)\n",
        "    train_labels = train.label.values[train_ids]\n",
        "    \n",
        "    val_inputs, val_masks = get_indices(X, val_ids) , get_indices(X_masks, val_ids)\n",
        "    val_labels = train.label.values[val_ids]\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    \n",
        "    val_inputs, val_labels, val_masks = list(zip(*sorted(zip(val_inputs, val_labels, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
        "    val_inputs, val_labels, val_masks = list(val_inputs), list(val_labels), list(val_masks)\n",
        "    \n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = BertDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = KSampler(train_data, batch_size)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = BertDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "    \n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "    train_fn(bert_classifier, train_dataloader, val_dataloader, fold= fold, epochs=3, evaluation=True, prefix=\"distill-multi_\")\n",
        "    \n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmopTlveSlBv"
      },
      "source": [
        "# Arabic Dialect Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ukGdJsjTSlBv"
      },
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 200, 3\n",
        "#768,100,3\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H, D_out),      \n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rPuMJ03hSlBw"
      },
      "source": [
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrlZnMgSlBw"
      },
      "source": [
        "<font size=\"3\"> This function is also important. It converts Latin letters to their Arabic counterparts using str.replace method in python. A light preprocessing is done before the replacement. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jz5grl3PSlBw"
      },
      "source": [
        "def convert(text):\n",
        "    \n",
        "    text = text.replace('',\"b\")\n",
        "    text = text.replace('',\"a\")\n",
        "    text = text.replace('',\"a\")\n",
        "    text = text.replace('',\"c\")\n",
        "    text = text.replace('',\"e\")\n",
        "    text = text.replace('',\"e\")\n",
        "    text = text.replace('$',\"s\")\n",
        "    text = text.replace(\"1\",\"\")\n",
        "    text = text.replace(\"\", \"u\")\n",
        "    \n",
        "    \n",
        "    text = text.lower()  #Make text lowercase, so Chibk and chbik is the same word\n",
        "    #Only accept alpha numeric letters, and some punctuation\n",
        "    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n",
        "\n",
        "    \n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)  #Keep hhhhh (Laughter) but for a limited length\n",
        "    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n",
        "    text = re.sub(r' [0-9]+ ', \" \", text)  #Remove standalone numbers\n",
        "    text = re.sub(r'^[0-9]+ ', \"\", text)\n",
        "    \n",
        "    \n",
        "    text = \" \" + text+ \" \"  #Add spaces and the end and the beginning\n",
        "\n",
        "\n",
        "    text = text.replace(\"ouw\", \"\")\n",
        "    text = text.replace(\"th\", \"\")\n",
        "    text = text.replace(\"kh\", \"\")\n",
        "    text = text.replace(\"ch\", \"\")\n",
        "    text = text.replace(\"ou\", \"\")\n",
        "    text = text.replace(\"aye\", \"\")\n",
        "    text = text.replace(\"dh\", \"\")\n",
        "    text = text.replace(\"bil\", \"\")\n",
        "    text = text.replace(\"ph\", \"\")\n",
        "    text = text.replace(\"iw\", \"\")\n",
        "    text = text.replace(\"sh\", \"\")\n",
        "    text = text.replace(\"ca\", \"\")\n",
        "    text = text.replace(\"ci\", \"\")\n",
        "    text = text.replace(\"ce\", \"\")\n",
        "    text = text.replace(\"co\", \"\")\n",
        "    text = text.replace(\"ck\", \"\")\n",
        "\n",
        "    text = text.replace(\" i\", \" \")\n",
        "    text = text.replace(\" a\", \" \")\n",
        "    text = text.replace(\" e\", \" \")\n",
        "    text = text.replace(\" o\", \" \")\n",
        "    \n",
        "    text = text.replace(\"a \", \" \")\n",
        "    text = text.replace(\"e \", \" \")\n",
        "    text = text.replace(\"i \", \" \")\n",
        "    text = text.replace(\"o \", \" \")\n",
        "    \n",
        "    text = text.replace(\"e\", \"\")\n",
        "    text = text.replace(\"a\", \"\")\n",
        "    text = text.replace(\"o\", \"\")\n",
        "\n",
        "    text = text.replace(\"b\", \"\")\n",
        "    text = text.replace(\"i\", \"\")\n",
        "    text = text.replace(\"k\", \"\")\n",
        "    text = text.replace(\"3\", \"\")\n",
        "    text = text.replace(\"5\", \"\")\n",
        "    text = text.replace(\"r\", \"\")\n",
        "    text = text.replace(\"4\", \"\")\n",
        "    text = text.replace(\"y\", \"\")\n",
        "    text = text.replace(\"s\", \"\")\n",
        "    text = text.replace(\"w\", \"\")\n",
        "    text = text.replace(\"m\", \"\")\n",
        "    text = text.replace(\"9\", \"\")\n",
        "    text = text.replace(\"n\",\"\")\n",
        "    text = text.replace(\"d\", \"\")\n",
        "    text = text.replace(\"l\" ,\"\")\n",
        "    text = text.replace(\"h\", \"\")\n",
        "    text = text.replace(\"7\", \"\")\n",
        "    text = text.replace(\"j\" ,\"\")\n",
        "    text = text.replace(\"t\", \"\")\n",
        "    text = text.replace(\"8\", \"\")\n",
        "    text = text.replace(\"2\", \"\")\n",
        "    text = text.replace(\"f\", \"\")\n",
        "    text = text.replace(\"p\", \"\")\n",
        "    text = text.replace(\"u\", \"\")\n",
        "    text = text.replace(\"g\", \"\")\n",
        "    text = text.replace(\"v\", \"\")\n",
        "    text = text.replace(\"c\", \"\")\n",
        "    text = text.replace(\"z\", \"\")\n",
        "    text = text.replace(\"q\", \"\")\n",
        "    text = text.replace(\"x\", \"\")\n",
        "    \n",
        "    \n",
        "    return text.strip()  #Strip from spaces at the beginnig and end\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WfdwVlHySlBx"
      },
      "source": [
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "train[\"text\"]=train['text'].apply(convert)\n",
        "test[\"text\"]=test['text'].apply(convert)\n",
        "\n",
        "\n",
        "\n",
        "model_name = 'moha/bert_ar_multi_dialect_c19'  #Load the Arabic dialect model \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-V_zm-_USlBx"
      },
      "source": [
        "X, X_masks = preprocessing_for_bert(train['text'].values, max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QrKT6YXLSlBx"
      },
      "source": [
        "#Due to the importance of this model in the ensemble, it is trained on five folds\n",
        "n = 10\n",
        "\n",
        "kfolds = KFold(n, True, 2020) \n",
        "fold = 0\n",
        "\n",
        "best_accs = []\n",
        "\n",
        "for train_ids, val_ids in kfolds.split(X):\n",
        "    \n",
        "    train_inputs, train_masks = get_indices(X, train_ids) , get_indices(X_masks, train_ids)\n",
        "    train_labels = train.label.values[train_ids]\n",
        "    \n",
        "    val_inputs, val_masks = get_indices(X, val_ids) , get_indices(X_masks, val_ids)\n",
        "    val_labels = train.label.values[val_ids]\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    \n",
        "    val_inputs, val_labels, val_masks = list(zip(*sorted(zip(val_inputs, val_labels, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
        "    val_inputs, val_labels, val_masks = list(val_inputs), list(val_labels), list(val_masks)\n",
        "    \n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    val_labels = torch.tensor(val_labels)\n",
        "    \n",
        "    # Create the DataLoader for our training set\n",
        "    train_data = BertDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = KSampler(train_data, batch_size)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data = BertDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
        "    \n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=3)\n",
        "    train_fn(bert_classifier, train_dataloader, val_dataloader, fold= fold, epochs=3, evaluation=True, prefix=\"ar_\")\n",
        "    \n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pcWNH8GlSlBx"
      },
      "source": [
        "print(\"Train time (minutes): %f\" % ((time.time() - start_time)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAI5vMvISlBx"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q2UqO8jNSlBy"
      },
      "source": [
        "#Utility function to predict using bert model\n",
        "def bert_single_predict(model, test_dataloader):\n",
        "\n",
        "    model.eval() #Turn on eval mode\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    for batch in tqdm(test_dataloader): #Iterate through the dataloader\n",
        "\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]  #Take to GPU\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)  #Perform inference, no grad\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()  #Convert them to probabilities and return\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "#Predict for every fold of the same model. It takes sentences (list) and models (a list of models of different folds).\n",
        "#It returns an array of probabilities of each model\n",
        "def bert_ensemble_predict(sentences, models, max_len=256):\n",
        "    \n",
        "    inputs, masks = preprocessing_for_bert(sentences, max_len=max_len)  #Preprocess\n",
        "    \n",
        "    \n",
        "    dataset = BertDataset(inputs, masks)  #Create the dataset, and the sequential sampler and dataloader\n",
        "    sample = SequentialSampler(dataset)\n",
        "    dataloader = DataLoader(dataset, sampler=sample, batch_size=128, collate_fn=data_collator)\n",
        "    \n",
        "    preds = []\n",
        "    \n",
        "    for model in models:  #Perform inference for each fold of the same base model\n",
        "        preds.append(bert_single_predict(model, dataloader))\n",
        "        \n",
        "    return preds "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CjLHTrU2SlBy"
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8TYnkgPgSlBy"
      },
      "source": [
        "#Fast ai inference\n",
        "fast_ai_probs, target = learn_classifier.get_preds(DatasetType.Test, ordered=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iqBokQEhSlBy"
      },
      "source": [
        "#Simple transformers inference\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "train, test = get_data()\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "\n",
        "pred = fast_model.predict(list(test['text']))\n",
        "simple_probs = softmax(pred[1],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SBJ9dWrOSlBz"
      },
      "source": [
        "#Arabic\n",
        "\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "test[\"text\"]=test['text'].apply(convert)  #Apply same transformation as training\n",
        "\n",
        "\n",
        "model_name = 'moha/bert_ar_multi_dialect_c19'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "lang_models = []\n",
        "for i in range(10):  #Load saved models, and append them to list\n",
        "    lang_models.append(torch.load(\"ar_best_\"+str(i), map_location=device))\n",
        "\n",
        "out = bert_ensemble_predict(test.text.tolist(), lang_models, max_len=512)\n",
        "\n",
        "\n",
        "arabic_probs = out[0]\n",
        "for i in range(1,10): #Sum up the probabilies of the different foldsd\n",
        "    arabic_probs = out[i] + arabic_probs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UjNNfw0SlBz"
      },
      "source": [
        "<font size=\"3\"> Same thing for the inference of the rest of the models </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IGGLsfwcSlBz"
      },
      "source": [
        "#Distill\n",
        "\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "\n",
        "\n",
        "model_name = 'distilbert-base-multilingual-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "lang_models = []\n",
        "for i in range(5):\n",
        "    lang_models.append(torch.load(\"distill-multi_best_\"+str(i), map_location=device))\n",
        "\n",
        "out = bert_ensemble_predict(test.text.tolist(), lang_models, max_len=512)\n",
        "\n",
        "\n",
        "distill_bert = out[0]\n",
        "for i in range(1,5):\n",
        "    distill_bert = out[i] + distill_bert\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4HWZFA7NSlBz"
      },
      "source": [
        "#French\n",
        "\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "\n",
        "model_name = 'camembert-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "lang_models = []\n",
        "for i in range(5):\n",
        "    lang_models.append(torch.load(\"fr_best_\"+str(i), map_location=device))\n",
        "\n",
        "out = bert_ensemble_predict(test.text.tolist(), lang_models, max_len=512)\n",
        "\n",
        "\n",
        "camembert_probs = out[0]\n",
        "for i in range(1,5):\n",
        "    camembert_probs = out[i] + camembert_probs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WrNVVcGpSlBz"
      },
      "source": [
        "#Multilingual\n",
        "\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "\n",
        "model_name = 'bert-base-multilingual-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "test[\"text\"]=test['text'].apply(lambda x :removeDuplicates(list(x.rstrip())) )\n",
        "\n",
        "\n",
        "lang_models = []\n",
        "for i in range(5):\n",
        "    lang_models.append(torch.load(\"multi-bert_best_\"+str(i), map_location=device))\n",
        "\n",
        "out = bert_ensemble_predict(test.text.tolist(), lang_models, max_len=512)\n",
        "\n",
        "\n",
        "multibert_probs = out[0]\n",
        "for i in range(1,5):\n",
        "    multibert_probs = out[i] + multibert_probs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7RpzWCfBSlB0"
      },
      "source": [
        "#Bert Uncased\n",
        "\n",
        "train, test = get_data()\n",
        "train.label+=1\n",
        "\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name ,do_lower_case=True)\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "test[\"text\"]=test['text'].apply(text_preprocessing)\n",
        "\n",
        "\n",
        "lang_models = []\n",
        "for i in range(5):\n",
        "    lang_models.append(torch.load(\"bert_best_\"+str(i), map_location=device))\n",
        "\n",
        "out = bert_ensemble_predict(test.text.tolist(), lang_models, max_len=512)\n",
        "\n",
        "\n",
        "bert_probs = out[0]\n",
        "for i in range(1,5):\n",
        "    bert_probs = out[i] + multibert_probs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRKIab8SlB0"
      },
      "source": [
        "<font size=\"3\"> <b> Ensemble formula </b> =  bert_probs\\*1.2 + multibert_probs\\*0.8 + camembert_probs + distill_bert\\*1.1 + arabic_probs\\*1.15 + fast_ai_probs\\*0.9 + simple_probs\\*0.9 </font>\n",
        "\n",
        "<font size=\"3\"> The weights signifies the importance of each model in the ensemble. Experimenting with different weight might yield a better model. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZhqzRywvSlB0"
      },
      "source": [
        "train, test = get_data()\n",
        "\n",
        "#Ensemble formula\n",
        "test[\"label\"] = (bert_probs*1.2+multibert_probs*0.8+camembert_probs+distill_bert*1.1+arabic_probs*1.15+fast_ai_probs.numpy()*0.9+simple_probs*0.9).argmax(1)-1\n",
        "test[[\"ID\", \"label\"]].to_csv(\"preds.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rSGG0qf1SlB0"
      },
      "source": [
        "print(\"Inference time (minutes): %f\" % ((time.time() - start_time)/60))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}